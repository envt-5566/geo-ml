{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ab7917-caaf-4016-89c0-fdac73e59390",
   "metadata": {},
   "source": [
    "# Practice exercise: Area of applicability \n",
    "\n",
    "This is a short practice exercise related to computing the area of applicability of a machine learning model (<a href=\"https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13650\" target=\"_blank\">Meyer and Pebesma, 2021</a>). You have access to a dataset of predictor variables derived from a <a href=\"https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR_HARMONIZED\" target=\"_blank\">Sentinel-2 satellite image</a> and topographic variables which correspond to points where there are leaf area index (LAI) measurements. The LAI measurements were derived from LiDAR data with a 30 cm spatial resolution and 15 cm vertical resolution averaged to a 10 m spatial resolution to match the size of Sentinel-2 pixels. \n",
    "\n",
    "LAI is a measure of the total area of leaves relative to the ground area and is an important biophysical variable for studying vegetation growth and functioning. The data we are using here were collected over the Marburg Forest in Germany and are from the paper by <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0304380019303230\" target=\"_blank\">Meyer et al. (2019)</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771264da-37ad-490c-acda-f8922462a0fc",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae044243-7233-4812-be32-b5042c88dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "if \"data-geoml\" not in os.listdir(os.getcwd()):\n",
    "    subprocess.run('wget \"https://github.com/envt-5566/geo-ml/raw/main/data/data-geoml.zip\"', shell=True, capture_output=True, text=True)\n",
    "    subprocess.run('unzip \"data-geoml.zip\"', shell=True, capture_output=True, text=True)\n",
    "    if \"data-geoml.zip\" not in os.listdir(os.getcwd()):\n",
    "        print(\"Has a directory called data-geoml been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n",
    "    else:\n",
    "        print(\"Data download OK\")\n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7834493c-fa2e-4db8-834d-a3ef9bce6d20",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfffbb53-8ca4-4e2d-b4a2-44b68c232a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install xarray[complete]\n",
    "    !pip install rioxarray\n",
    "    !pip install mapclassify\n",
    "    !pip install contextily\n",
    "    !pip install pysal\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from math import log\n",
    "from tqdm import tqdm\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import contextily as cx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# models\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d42b8-a9b5-44c3-a2ff-6eba43f32a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(os.path.join(DATA_PATH, \"lai_meyer_et_al_2019_marburg.geojson\"))\n",
    "gdf = gdf.drop(columns=[\"field_1\", \"ID\", \"x_utm_25832\", \"y_utm_25832\"])\n",
    "gdf = gdf.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52792021-0836-4ce7-8db8-aad8eb5027e3",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "To compute the area of applicability of a machine learning model we need to i) measure how similar the predictor variables are at a candidate location for prediction to the training data used to develop the model; <a href=\"https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13650\" target=\"_blank\">Meyer and Pebesma (2021)</a> present a dissimilarity index for this task, and ii) determine a threshold dissimilarity index value which delineates the coverage of the training dataset; again <a href=\"https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13650\" target=\"_blank\">Meyer and Pebesma (2021)</a> present a method for this. **Can you adapt the examples from the *area of applicability* notebook to compute the dissimilarity index threshold for the training dataset provided to compute LAI?**\n",
    "\n",
    "You will need to consider:\n",
    "\n",
    "* What variables to drop before model training.\n",
    "* Do you need to fit or train a model? If not, you can pass `None` is as the model argument to `compute_di_threshold()`.\n",
    "* Should you use random or spatial k-fold cross-validation.\n",
    "\n",
    "We have copied over the functions from the *area of applicability* notebook to help you. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f472851-31eb-4d86-9c20-cae4556996a6",
   "metadata": {},
   "source": [
    "### Dissimilarity Index\n",
    "\n",
    "The dissimilarity index is a measure of the distance between a candidate location for prediction using the machine learning model and the model's training data. It measures distance in predictor variable space and is a measure of how similar the predictor variable values are to the most similar example in the training dataset. The process to compute the dissimilarity index is:\n",
    "\n",
    "1. Compute the Euclidean distance between the predictor variables for the candidate location and every example in the training dataset.\n",
    "2. Select the minimum distance.\n",
    "3. Compute the average Euclidean distance between predictor variables for all examples in the training dataset.\n",
    "4. Divide the minimum distance between the candidate prediction location and the training dataset (2) by the average distance between predictor variables in the training dataset (3).\n",
    "\n",
    "The dissimilarity index for the candidate prediction location is the result of step 4. Standardising the dissimilarity index values by dividing by the average distance between predictor variables in the training dataset means that dissimilarity index values range from 0 to infinity. If the dissimilarity index value is 0 then the candidate prediction location is identical to an example in the training dataset.\n",
    "\n",
    "The following is a helper function that implements steps 2.1, 2.3 and 2.4 in Meyer and Pebesma (2021) and computes the dissimilarity index given arrays of predictor variables for the training dataset and candidate predictor locations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b76b0e-4f89-47bb-8cd7-1d6667bcca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_di(X_train, X_preds, dbar, scaled):\n",
    "    \"\"\"Compute the Dissimilarity Index (DI).\n",
    "\n",
    "    This is based on steps 2.1, 2.3, and 2.4 in Meyer and Pebesma (2021).\n",
    "\n",
    "    Note, we omit the weighting of predictor variables (step 2.2 of Meyer and Pebesma (2021).\n",
    "    \n",
    "    We implement the steps explicitly (i.e. using for loops rather than NumPy broadcasting\n",
    "    and linear algebra operations). This is for education purposes to support understanding how\n",
    "    DI is computed. \n",
    "\n",
    "    Args:\n",
    "        X_train (ndarray): Array of training data where examples are aligned on the 0 axis and features on the 1 axis.\n",
    "        X_preds (ndarray): Array of predictor data where examples are aligned on the 0 axis and features on the 1 axis.\n",
    "        dbar (float | None): Precomputed mean of dissimilarity index for all values in the training dataset.\n",
    "        scaled (bool): Flag to indicate if input data is already standardised.\n",
    "\n",
    "    Returns:\n",
    "        ndarray, an array of DI values for each example in the prediction set. \n",
    "    \"\"\"\n",
    "    \n",
    "    # standardise predictors (using training data)\n",
    "    if scaled:\n",
    "        X_train_scaled = X_train\n",
    "        X_preds_scaled = X_preds\n",
    "    else:\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_preds_scaled = scaler.transform(X_preds)\n",
    "\n",
    "    # Compute dissimilarity index (DI)\n",
    "    # Section 2.3 in Meyer and Pebesma (2021)\n",
    "    \n",
    "    # create a matrix to store Euclidean distance between \n",
    "    # all training points and prediction points\n",
    "    n_X_train = X_train_scaled.shape[0]\n",
    "    n_X_preds = X_preds_scaled.shape[0]\n",
    "\n",
    "    d_arr = np.zeros((n_X_train, n_X_preds))\n",
    "\n",
    "    for r in range(0, n_X_train):\n",
    "        x_train = X_train_scaled[r, :]\n",
    "        for c in range(0, n_X_preds):\n",
    "            x_preds = X_preds_scaled[c, :]\n",
    "            # compute Euclidean distance\n",
    "            x_dist = np.sqrt(np.sum((x_train - x_preds) * (x_train - x_preds)))\n",
    "            # store Euclidean distance in d_arr\n",
    "            d_arr[r, c] = x_dist\n",
    "            \n",
    "    # get the minimum distance between each point in x_preds\n",
    "    # and the training set\n",
    "    d_arr = np.min(d_arr, axis=0)\n",
    "    \n",
    "    # Section 2.4 in Meyer and Pebesma (2021)\n",
    "    if dbar:\n",
    "        di = d_arr / dbar\n",
    "    else:\n",
    "        # compute average distance between all\n",
    "        # training points to standardise DI\n",
    "        d_train_arr = []\n",
    "        for r in range(0, n_X_train):\n",
    "            x_train_1 = X_train_scaled[r, :]\n",
    "            for c in range(r+1, n_X_train):\n",
    "                x_train_2 = X_train_scaled[c, :]\n",
    "                # compute Euclidean distance\n",
    "                x_dist = np.sqrt(np.sum((x_train_1 - x_train_2) * (x_train_1 - x_train_2)))\n",
    "                # store Euclidean distance in d_arr\n",
    "                d_train_arr.append(x_dist)\n",
    "    \n",
    "        di = d_arr / np.mean(d_train_arr)\n",
    "\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbdbf5-fd6e-4664-a100-d3e4a5c47c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dbar(X_train, scaled):\n",
    "    \"\"\"Compute average DI between all values in the training dataset.\n",
    "    \n",
    "    d_bar is computed section 2.4 in Meyer and Pebesma (2021). It is \n",
    "    used to standardise dissimilarity index values. \n",
    "\n",
    "    Precomputing can save time when the training dataset is fixed and we\n",
    "    want to iterate over a range of candidate prediction locations.\n",
    "\n",
    "    We implement the steps explicitly (i.e. using for loops rather than NumPy broadcasting\n",
    "    and linear algebra operations). This is for education purposes to support understanding how\n",
    "    dbar is computed. \n",
    "    \"\"\"\n",
    "    # standardise predictors (using training data)\n",
    "    if scaled:\n",
    "        X_train_scaled = X_train\n",
    "    else:\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "    n_X_train = X_train.shape[0]\n",
    "        \n",
    "    # Section 2.4 in Meyer and Pebesma (2021)\n",
    "    \n",
    "    # compute average distance between all\n",
    "    # training points to standardise DI\n",
    "    d_train_arr = []\n",
    "    for r in range(0, n_X_train):\n",
    "        x_train_1 = X_train_scaled[r, :]\n",
    "        for c in range(r+1, n_X_train):\n",
    "            x_train_2 = X_train_scaled[c, :]\n",
    "            # compute Euclidean distance\n",
    "            x_dist = np.sqrt(np.sum((x_train_1 - x_train_2) * (x_train_1 - x_train_2)))\n",
    "            # store Euclidean distance in d_arr\n",
    "            d_train_arr.append(x_dist)\n",
    "            \n",
    "    return np.mean(d_train_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f53c2f-bed1-4339-8816-a1ece6b83984",
   "metadata": {},
   "source": [
    "### Dissimilarity index threshold\n",
    "\n",
    "Meyer and Pebesma (2021) in section 2.5 compute a dissimilarity index threshold which represents the outlier removed maximum dissimilarity of the training dataset. This represents a threshold for the model's area of applicability. If a candidate prediction location has a dissimilarity index greater than this threshold it is deemed outside the model's area of applicability. \n",
    "\n",
    "The dissimilarity index threshold is computed using k-fold cross-validation on the training dataset:\n",
    "\n",
    "1. Split the training dataset into $k$ folds.\n",
    "2. Hold out each $k$ fold in-turn:\n",
    "    * Compute the dissimilarity index between all values in the hold-out fold and training folds.\n",
    "    * Optional step: Compute model predictions and error for all values in the hold-out fold.\n",
    "3. Compute the outlier-removed threshold as the 75th percentile dissimilarity index value + (1.5 * IQR)\n",
    "\n",
    "The following helper function computes the dissimilarity index threshold given the training data as a `GeoDataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bebf9d-118e-4427-9f0e-5d6a215bece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_di_theshold(gdf, n_folds, model, spatial_cv, target_column, cols_to_drop):\n",
    "    \"\"\"\n",
    "    Function to compute DI threshold following Meyer and Pebesma (2021).\n",
    "\n",
    "    The DI threshold is computed as the 75th percentile + (1.5 * IQR). DI threshold is \n",
    "    considered the outlier removed maximum DI of the training data set. Data points with a \n",
    "    DI greater than the DI threshold are outside the area of applicability of the model\n",
    "    given the training data. \n",
    "\n",
    "    We implement the steps explicitly (i.e. using for loops rather than NumPy broadcasting\n",
    "    and linear algebra operations). This is for education purposes to support understanding how\n",
    "    the DI threshold is computed. \n",
    "    \n",
    "    Args:\n",
    "        gdf (GeoDataFrame): The spatial dataset (GeoDataFrame) containing features and the target variable.\n",
    "        n_folds (int): The number of folds to use for cross-validation.\n",
    "        model: The machine learning model to train and evaluate.\n",
    "        spatial_cv (bool): If `True`, use k-fold spatial cross-validation. Otherwise, use k-fold random cross-validation.\n",
    "        target_column (str): The column name for the target variable.\n",
    "        cols_to_drop (list): Columns to drop that should not be included as predictors.\n",
    "\n",
    "    Returns:\n",
    "        dict, dict object with the DI threshold, the DI values for the hold out folds, the errors\n",
    "        for the hold out folds, and the predictions for the hold out folds.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Section 2.5 in Meyer and Pebesma (2021)\n",
    "    if spatial_cv:\n",
    "        gdf_tmp = gdf.copy()\n",
    "\n",
    "        # split the data into n_folds spatially\n",
    "        # assign a fold id column to each row\n",
    "    \n",
    "        # add x and y columns\n",
    "        gdf_tmp.loc[:, \"x\"] = gdf_tmp.loc[:, \"geometry\"].x\n",
    "        gdf_tmp.loc[:, \"y\"] = gdf_tmp.loc[:, \"geometry\"].y\n",
    "    \n",
    "        # create an array of x and y values\n",
    "        X = gdf_tmp.loc[:, [\"x\", \"y\"]].copy()\n",
    "    \n",
    "        # create a KMeans clusterer\n",
    "        km = KMeans(\n",
    "            n_clusters=n_folds,\n",
    "            init=\"random\",\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=123,\n",
    "        )\n",
    "        spatial_folds = km.fit_predict(X)\n",
    "        gdf_tmp.loc[:, \"fold\"] = spatial_folds\n",
    "    \n",
    "        # keep a copy of GeoDataFrame with folds for plotting\n",
    "        gdf_spatial_folds = gdf_tmp.copy()\n",
    "\n",
    "        # drop columns that should not be included as predictors\n",
    "        gdf_tmp = gdf_tmp.drop(columns=cols_to_drop)\n",
    "        \n",
    "    else:\n",
    "        gdf_tmp = gdf.copy()\n",
    "\n",
    "        # split the data into n_folds\n",
    "        # assign a fold id column to each row\n",
    "        # shuffle the GeoDataFrame\n",
    "        gdf_tmp = gdf_tmp.sample(frac=1, random_state=123)\n",
    "        \n",
    "        # assign a fold id using the modulo operation\n",
    "        gdf_tmp.loc[:, \"fold\"] = np.arange(len(gdf_tmp)) % n_folds\n",
    "        \n",
    "        # store shuffled gdf to return for plotting\n",
    "        gdf_folds = gdf_tmp.copy()\n",
    "\n",
    "        # drop columns that should not be included as predictors\n",
    "        gdf_tmp = gdf_tmp.drop(columns=cols_to_drop)\n",
    "\n",
    "    # list to store DI's computed on the hold out folds\n",
    "    dis = []\n",
    "\n",
    "    # predictions on cv hold out folds\n",
    "    predictions = []\n",
    "    \n",
    "    # errors on cv hold out folds\n",
    "    errors = []\n",
    "    \n",
    "    # loop over folds and compute DI\n",
    "    for k in range(0, n_folds):\n",
    "        # subset out preds fold\n",
    "        gdf_preds_fold = gdf_tmp.loc[gdf_tmp[\"fold\"] == k, :]\n",
    "        \n",
    "        # subset out training folds\n",
    "        # != is the not equals to operator\n",
    "        gdf_train_fold = gdf_tmp.loc[gdf_tmp[\"fold\"] != k, :]\n",
    "\n",
    "        # subset training folds into predictors and targets arrays\n",
    "        X_train = gdf_train_fold.drop(columns=[target_column, \"fold\"])\n",
    "        y_train = gdf_train_fold.loc[:, target_column]\n",
    "\n",
    "        # subset preds fold into predictors and targets arrays\n",
    "        X_preds = gdf_preds_fold.drop(columns=[target_column, \"fold\"])\n",
    "        y_preds = gdf_preds_fold.loc[:, target_column]\n",
    "\n",
    "        # standardise data\n",
    "        tmp_scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = tmp_scaler.transform(X_train)\n",
    "        X_preds_scaled = tmp_scaler.transform(X_preds)\n",
    "\n",
    "        # use compute_df function previously defined\n",
    "        fold_dis = compute_di(X_train_scaled, X_preds_scaled, None, scaled=True)\n",
    "\n",
    "        dis = dis + fold_dis.tolist()\n",
    "\n",
    "        # train model \n",
    "        if model:\n",
    "            m = model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "            # compute predictions and error using the hold out fold \n",
    "            y_test_preds = m.predict(X_preds_scaled)\n",
    "            predictions = predictions + y_test_preds.tolist()\n",
    "    \n",
    "            y_preds_errors = y_test_preds - y_preds\n",
    "            errors = errors + y_preds_errors.tolist()\n",
    "\n",
    "    # compute DI threshold\n",
    "    # 75th percentile 1.5 * IQR as in Meyer and Pebesma (2021)\n",
    "    iqr = np.percentile(dis, 75) - np.percentile(dis, 25)\n",
    "    di_threshold = np.percentile(dis, 75) + (1.5 * iqr)\n",
    "\n",
    "    output = {}\n",
    "    output[\"di_threshold\"] = di_threshold\n",
    "    output[\"di\"] = dis\n",
    "    output[\"targets\"] = y_preds\n",
    "    output[\"predictions\"] = predictions\n",
    "    output[\"errors\"] = errors\n",
    "\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
