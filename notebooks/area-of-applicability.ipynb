{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee12a638-1176-4705-a707-45b9cdec2137",
   "metadata": {},
   "source": [
    "# Machine learning model area of applicability\n",
    "\n",
    "Once a machine learning model has been trained, it is important to assess where it can be used to generate predicted maps. Machine learning models learn relationships between predictor variables and target values using a training dataset. The model should only be used to generate predictions in locations where the learnt relationships between predictor variables and target values hold. Before using a model, we need to identify the area where the model is applicable given its training data. Relatedly, a model is evaluated using a performance metric (e.g. root mean squared error (RMSE)). We also need to know for what area that estimate of model error is applicable how model error varies with increasing distance from locations covered in the training data.  \n",
    "\n",
    "A paper by <a href=\"https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13650\" target=\"_blank\">Meyer and Pebesma (2021)</a> addresses these issues and presents a method for computing the area of applicability for a machine learning model. Their method computes a dissimilarity index measuring the distance between the values of predictor variables in a candidate location for prediction using the model and values of predictor variables in the model's training dataset. If the dissimilarity index value is greater than a threshold it is deemed outside the model's area of applicability. They also present a cross-validation method for assessing the relationship between dissimilarity index values and model error. \n",
    "\n",
    "This notebook steps through the methods presented by <a href=\"https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13650\" target=\"_blank\">Meyer and Pebesma (2021)</a> and demonstrates how we can compute the area of applicability for a machine learning model that predicts plant species richness from bioclimatic and elevation variables that was trained on data collected at points across South America. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b27f7-6518-4f59-9442-c2f95eee6ab7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f1b21-d846-4ce1-9c4b-38983e84654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "if \"data-geoml\" not in os.listdir(os.getcwd()):\n",
    "    subprocess.run('wget \"https://github.com/envt-5566/geo-ml/raw/main/data/data-geoml.zip\"', shell=True, capture_output=True, text=True)\n",
    "    subprocess.run('unzip \"data-geoml.zip\"', shell=True, capture_output=True, text=True)\n",
    "    if \"data-geoml.zip\" not in os.listdir(os.getcwd()):\n",
    "        print(\"Has a directory called data-geoml been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n",
    "    else:\n",
    "        print(\"Data download OK\")\n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd90b1-b5b2-42eb-9124-1910669bb01c",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b52b566-f54a-440d-8e6d-e24d36b6bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install xarray[complete]\n",
    "    !pip install rioxarray\n",
    "    !pip install mapclassify\n",
    "    !pip install contextily\n",
    "    !pip install pysal\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from math import log\n",
    "from tqdm import tqdm\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import contextily as cx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# models\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbb069a-1541-463e-95d6-84e051de1773",
   "metadata": {},
   "source": [
    "## Dissimilarity Index\n",
    "\n",
    "The dissimilarity index is a measure of the distance between a candidate location for prediction using the machine learning model and the model's training data. It measures distance in predictor variable space and is a measure of how similar the predictor variable values are to the most similar example in the training dataset. The process to compute the dissimilarity index is:\n",
    "\n",
    "1. Compute the Euclidean distance between the predictor variables for the candidate location and every example in the training dataset.\n",
    "2. Select the minimum distance.\n",
    "3. Compute the average Euclidean distance between predictor variables for all examples in the training dataset.\n",
    "4. Divide the minimum distance between the candidate prediction location and the training dataset (2) by the average distance between predictor variables in the training dataset (3).\n",
    "\n",
    "The dissimilarity index for the candidate prediction location is the result of step 4. Standardising the dissimilarity index values by dividing by the average distance between predictor variables in the training dataset means that dissimilarity index values range from 0 to infinity. If the dissimilarity index value is 0 then the candidate prediction location is identical to an example in the training dataset.\n",
    "\n",
    "The following is a helper function that implements steps 2.1, 2.3 and 2.4 in Meyer and Pebesma (2021) and computes the dissimilarity index given arrays of predictor variables for the training dataset and candidate predictor locations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87ed5f-6d4c-4e81-8cf3-c2fcd5a5039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_di(X_train, X_preds, dbar, scaled):\n",
    "    \"\"\"Compute the Dissimilarity Index (DI).\n",
    "\n",
    "    This is based on steps 2.1, 2.3, and 2.4 in Meyer and Pebesma (2021).\n",
    "\n",
    "    Note, we omit the weighting of predictor variables (step 2.2 of Meyer and Pebesma (2021).\n",
    "    \n",
    "    We implement the steps explicitly (i.e. using for loops rather than NumPy broadcasting\n",
    "    and linear algebra operations). This is for education purposes to support understanding how\n",
    "    DI is computed. \n",
    "\n",
    "    Args:\n",
    "        X_train (ndarray): Array of training data where examples are aligned on the 0 axis and features on the 1 axis.\n",
    "        X_preds (ndarray): Array of predictor data where examples are aligned on the 0 axis and features on the 1 axis.\n",
    "        dbar (float | None): Precomputed mean of dissimilarity index for all values in the training dataset.\n",
    "        scaled (bool): Flag to indicate if input data is already standardised.\n",
    "\n",
    "    Returns:\n",
    "        ndarray, an array of DI values for each example in the prediction set. \n",
    "    \"\"\"\n",
    "    \n",
    "    # standardise predictors (using training data)\n",
    "    if scaled:\n",
    "        X_train_scaled = X_train\n",
    "        X_preds_scaled = X_preds\n",
    "    else:\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_preds_scaled = scaler.transform(X_preds)\n",
    "\n",
    "    # Compute dissimilarity index (DI)\n",
    "    # Section 2.3 in Meyer and Pebesma (2021)\n",
    "    \n",
    "    # create a matrix to store Euclidean distance between \n",
    "    # all training points and prediction points\n",
    "    n_X_train = X_train_scaled.shape[0]\n",
    "    n_X_preds = X_preds_scaled.shape[0]\n",
    "\n",
    "    d_arr = np.zeros((n_X_train, n_X_preds))\n",
    "\n",
    "    for r in range(0, n_X_train):\n",
    "        x_train = X_train_scaled[r, :]\n",
    "        for c in range(0, n_X_preds):\n",
    "            x_preds = X_preds_scaled[c, :]\n",
    "            # compute Euclidean distance\n",
    "            x_dist = np.sqrt(np.sum((x_train - x_preds) * (x_train - x_preds)))\n",
    "            # store Euclidean distance in d_arr\n",
    "            d_arr[r, c] = x_dist\n",
    "            \n",
    "    # get the minimum distance between each point in x_preds\n",
    "    # and the training set\n",
    "    d_arr = np.min(d_arr, axis=0)\n",
    "    \n",
    "    # Section 2.4 in Meyer and Pebesma (2021)\n",
    "    if dbar:\n",
    "        di = d_arr / dbar\n",
    "    else:\n",
    "        # compute average distance between all\n",
    "        # training points to standardise DI\n",
    "        d_train_arr = []\n",
    "        for r in range(0, n_X_train):\n",
    "            x_train_1 = X_train_scaled[r, :]\n",
    "            for c in range(r+1, n_X_train):\n",
    "                x_train_2 = X_train_scaled[c, :]\n",
    "                # compute Euclidean distance\n",
    "                x_dist = np.sqrt(np.sum((x_train_1 - x_train_2) * (x_train_1 - x_train_2)))\n",
    "                # store Euclidean distance in d_arr\n",
    "                d_train_arr.append(x_dist)\n",
    "    \n",
    "        di = d_arr / np.mean(d_train_arr)\n",
    "\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a0e25-258e-4d6e-8f6a-2e3dcb8ab3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dbar(X_train, scaled):\n",
    "    \"\"\"Compute average DI between all values in the training dataset.\n",
    "    \n",
    "    d_bar is computed section 2.4 in Meyer and Pebesma (2021). It is \n",
    "    used to standardise dissimilarity index values. \n",
    "\n",
    "    Precomputing can save time when the training dataset is fixed and we\n",
    "    want to iterate over a range of candidate prediction locations.\n",
    "\n",
    "    We implement the steps explicitly (i.e. using for loops rather than NumPy broadcasting\n",
    "    and linear algebra operations). This is for education purposes to support understanding how\n",
    "    dbar is computed. \n",
    "    \"\"\"\n",
    "    # standardise predictors (using training data)\n",
    "    if scaled:\n",
    "        X_train_scaled = X_train\n",
    "    else:\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "    n_X_train = X_train.shape[0]\n",
    "        \n",
    "    # Section 2.4 in Meyer and Pebesma (2021)\n",
    "    \n",
    "    # compute average distance between all\n",
    "    # training points to standardise DI\n",
    "    d_train_arr = []\n",
    "    for r in range(0, n_X_train):\n",
    "        x_train_1 = X_train_scaled[r, :]\n",
    "        for c in range(r+1, n_X_train):\n",
    "            x_train_2 = X_train_scaled[c, :]\n",
    "            # compute Euclidean distance\n",
    "            x_dist = np.sqrt(np.sum((x_train_1 - x_train_2) * (x_train_1 - x_train_2)))\n",
    "            # store Euclidean distance in d_arr\n",
    "            d_train_arr.append(x_dist)\n",
    "            \n",
    "    return np.mean(d_train_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e9ec0-268c-46cc-bff2-0923a0614377",
   "metadata": {},
   "source": [
    "## Dissimilarity index threshold\n",
    "\n",
    "Meyer and Pebesma (2021) in section 2.5 compute a dissimilarity index threshold which represents the outlier removed maximum dissimilarity of the training dataset. This represents a threshold for the model's area of applicability. If a candidate prediction location has a dissimilarity index greater than this threshold it is deemed outside the model's area of applicability. \n",
    "\n",
    "The dissimilarity index threshold is computed using k-fold cross-validation on the training dataset:\n",
    "\n",
    "1. Split the training dataset into $k$ folds.\n",
    "2. Hold out each $k$ fold in-turn:\n",
    "    * Compute the dissimilarity index between all values in the hold-out fold and training folds.\n",
    "    * Optional step: Compute model predictions and error for all values in the hold-out fold.\n",
    "3. Compute the outlier-removed threshold as the 75th percentile dissimilarity index value + (1.5 * IQR)\n",
    "\n",
    "The following helper function computes the dissimilarity index threshold given the training data as a `GeoDataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed208e-4c67-4468-a665-da2bd01f6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_di_theshold(gdf, n_folds, model, spatial_cv, target_column, cols_to_drop):\n",
    "    \"\"\"\n",
    "    Function to compute DI threshold following Meyer and Pebesma (2021).\n",
    "\n",
    "    The DI threshold is computed as the 75th percentile + (1.5 * IQR). DI threshold is \n",
    "    considered the outlier removed maximum DI of the training data set. Data points with a \n",
    "    DI greater than the DI threshold are outside the area of applicability of the model\n",
    "    given the training data. \n",
    "\n",
    "    We implement the steps explicitly (i.e. using for loops rather than NumPy broadcasting\n",
    "    and linear algebra operations). This is for education purposes to support understanding how\n",
    "    the DI threshold is computed. \n",
    "    \n",
    "    Args:\n",
    "        gdf (GeoDataFrame): The spatial dataset (GeoDataFrame) containing features and the target variable.\n",
    "        n_folds (int): The number of folds to use for cross-validation.\n",
    "        model: The machine learning model to train and evaluate.\n",
    "        spatial_cv (bool): If `True`, use k-fold spatial cross-validation. Otherwise, use k-fold random cross-validation.\n",
    "        target_column (str): The column name for the target variable.\n",
    "        cols_to_drop (list): Columns to drop that should not be included as predictors.\n",
    "\n",
    "    Returns:\n",
    "        dict, dict object with the DI threshold, the DI values for the hold out folds, the errors\n",
    "        for the hold out folds, and the predictions for the hold out folds.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Section 2.5 in Meyer and Pebesma (2021)\n",
    "    if spatial_cv:\n",
    "        gdf_tmp = gdf.copy()\n",
    "\n",
    "        # split the data into n_folds spatially\n",
    "        # assign a fold id column to each row\n",
    "    \n",
    "        # add x and y columns\n",
    "        gdf_tmp.loc[:, \"x\"] = gdf_tmp.loc[:, \"geometry\"].x\n",
    "        gdf_tmp.loc[:, \"y\"] = gdf_tmp.loc[:, \"geometry\"].y\n",
    "    \n",
    "        # create an array of x and y values\n",
    "        X = gdf_tmp.loc[:, [\"x\", \"y\"]].copy()\n",
    "    \n",
    "        # create a KMeans clusterer\n",
    "        km = KMeans(\n",
    "            n_clusters=n_folds,\n",
    "            init=\"random\",\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=123,\n",
    "        )\n",
    "        spatial_folds = km.fit_predict(X)\n",
    "        gdf_tmp.loc[:, \"fold\"] = spatial_folds\n",
    "    \n",
    "        # keep a copy of GeoDataFrame with folds for plotting\n",
    "        gdf_spatial_folds = gdf_tmp.copy()\n",
    "\n",
    "        # drop columns that should not be included as predictors\n",
    "        gdf_tmp = gdf_tmp.drop(columns=cols_to_drop)\n",
    "        \n",
    "    else:\n",
    "        gdf_tmp = gdf.copy()\n",
    "\n",
    "        # split the data into n_folds\n",
    "        # assign a fold id column to each row\n",
    "        # shuffle the GeoDataFrame\n",
    "        gdf_tmp = gdf_tmp.sample(frac=1, random_state=123)\n",
    "        \n",
    "        # assign a fold id using the modulo operation\n",
    "        gdf_tmp.loc[:, \"fold\"] = np.arange(len(gdf_tmp)) % n_folds\n",
    "        \n",
    "        # store shuffled gdf to return for plotting\n",
    "        gdf_folds = gdf_tmp.copy()\n",
    "\n",
    "        # drop columns that should not be included as predictors\n",
    "        gdf_tmp = gdf_tmp.drop(columns=cols_to_drop)\n",
    "\n",
    "    # list to store DI's computed on the hold out folds\n",
    "    dis = []\n",
    "\n",
    "    # predictions on cv hold out folds\n",
    "    predictions = []\n",
    "    \n",
    "    # errors on cv hold out folds\n",
    "    errors = []\n",
    "    \n",
    "    # loop over folds and compute DI\n",
    "    for k in range(0, n_folds):\n",
    "        # subset out preds fold\n",
    "        gdf_preds_fold = gdf_tmp.loc[gdf_tmp[\"fold\"] == k, :]\n",
    "        \n",
    "        # subset out training folds\n",
    "        # != is the not equals to operator\n",
    "        gdf_train_fold = gdf_tmp.loc[gdf_tmp[\"fold\"] != k, :]\n",
    "\n",
    "        # subset training folds into predictors and targets arrays\n",
    "        X_train = gdf_train_fold.drop(columns=[target_column, \"fold\"])\n",
    "        y_train = gdf_train_fold.loc[:, target_column]\n",
    "\n",
    "        # subset preds fold into predictors and targets arrays\n",
    "        X_preds = gdf_preds_fold.drop(columns=[target_column, \"fold\"])\n",
    "        y_preds = gdf_preds_fold.loc[:, target_column]\n",
    "\n",
    "        # standardise data\n",
    "        tmp_scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = tmp_scaler.transform(X_train)\n",
    "        X_preds_scaled = tmp_scaler.transform(X_preds)\n",
    "\n",
    "        # use compute_df function previously defined\n",
    "        fold_dis = compute_di(X_train_scaled, X_preds_scaled, None, scaled=True)\n",
    "\n",
    "        dis = dis + fold_dis.tolist()\n",
    "\n",
    "        # train model \n",
    "        if model:\n",
    "            m = model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "            # compute predictions and error using the hold out fold \n",
    "            y_test_preds = m.predict(X_preds_scaled)\n",
    "            predictions = predictions + y_test_preds.tolist()\n",
    "    \n",
    "            y_preds_errors = y_test_preds - y_preds\n",
    "            errors = errors + y_preds_errors.tolist()\n",
    "\n",
    "    # compute DI threshold\n",
    "    # 75th percentile 1.5 * IQR as in Meyer and Pebesma (2021)\n",
    "    iqr = np.percentile(dis, 75) - np.percentile(dis, 25)\n",
    "    di_threshold = np.percentile(dis, 75) + (1.5 * iqr)\n",
    "\n",
    "    output = {}\n",
    "    output[\"di_threshold\"] = di_threshold\n",
    "    output[\"di\"] = dis\n",
    "    output[\"targets\"] = y_preds\n",
    "    output[\"predictions\"] = predictions\n",
    "    output[\"errors\"] = errors\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5182cb-982e-4933-af29-fdb470a0b738",
   "metadata": {},
   "source": [
    "## Compute the area of applicability\n",
    "\n",
    "We will compute the area of applicability for a machine learning model that predicts plant species richness at a location using bioclimatic variables and elevation as predictors. Let's load the labelled data and predictors and plot them on a web map. You can see the points are not evenly distributed across South America; this should raise concerns about the ability of the model to generate predictions in locations far away from the training data. To assess where we can use a machine learning model trained on this data to predict species richness, we'll compute the area of applicability for this model. Here, we'll focus on assessing which locations within Chile and parts of Agrentina are applicable for mapping using our model. \n",
    "\n",
    "This is the dataset presented in <a href=\"https://arxiv.org/html/2404.06978v1\" target=\"_blank\">Meyer et al. (2024)</a> that includes points across South America representing vegetation surveys where species richness counts were recorded from the <a href=\"https://onlinelibrary.wiley.com/doi/10.1111/geb.13346\" target=\"_blank\">sPlotOpen database</a>. Species richness counts are the target values. Predictor variables included with this dataset are <a href=\"https://developers.google.com/earth-engine/datasets/catalog/WORLDCLIM_V1_BIO#bands\" target=\"_blank\">WorldClim bioclimatic variables</a> and elevation. The bioclimatic variables have the following definitions:\n",
    "\n",
    "* BIO1: Annual Mean Temperature\n",
    "* BIO2: Mean Diurnal Range (Mean of monthly (max temp - min temp))\n",
    "* BIO3: Isothermality (BIO2/BIO7) (×100)\n",
    "* BIO4: Temperature Seasonality (standard deviation ×100)\n",
    "* BIO5: Max Temperature of Warmest Month\n",
    "* BIO6: Min Temperature of Coldest Month\n",
    "* BIO7: Temperature Annual Range (BIO5-BIO6)\n",
    "* BIO8: Mean Temperature of Wettest Quarter\n",
    "* BIO9: Mean Temperature of Driest Quarter\n",
    "* BIO10: Mean Temperature of Warmest Quarter\n",
    "* BIO11: Mean Temperature of Coldest Quarter\n",
    "* BIO12: Annual Precipitation\n",
    "* BIO13: Precipitation of Wettest Month\n",
    "* BIO14: Precipitation of Driest Month\n",
    "* BIO15: Precipitation Seasonality (Coefficient of Variation)\n",
    "* BIO16: Precipitation of Wettest Quarter\n",
    "* BIO17: Precipitation of Driest Quarter\n",
    "* BIO18: Precipitation of Warmest Quarter\n",
    "* BIO19: Precipitation of Coldest Quarter\n",
    "\n",
    "Let's start by loading and mapping the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06176c9-ef35-4803-99cf-567423dc528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(os.path.join(DATA_PATH, \"plant_species_south_america.gpkg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf03b5-5a4e-48b1-863b-d5a37949c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.explore(column=\"Species_richness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef613635-e8bf-4b59-a58c-439c792a0a35",
   "metadata": {},
   "source": [
    "Let's now load some predictor variables which we we'd like to feed into our machine learning model to generate a predicted map of plant species richness. The predictor variables are stored as raster format data. Each predictor variable is stored as an array of pixels with each pixel representing a location of the Earth's surface. The predictor variables that we'll load here cover Chile and parts of Argentina and are stored in a GeoTIFF file. \n",
    "\n",
    "We'll load the GeoTIFF file into our program as an xarray `DataArray`. \n",
    "\n",
    "### Quick aside: Xarray\n",
    "\n",
    "Xarray is a Python package that builds on top of NumPy's array-based data structures, but provides extra tools and functions that are useful for working with geospatial and Earth Science datasets. For example, `xarray.DataArray` data structures are objects that store multidimensional arrays of raster values and also store metadata information that describe the raster values. \n",
    "\n",
    "Xarray also provides convenient functions for reading raster data from geospatial data files on disk into memory as `xarray.DataArray` objects which we can use in our Python programs while retaining geographic and temporal information about the raster values stored in the array.\n",
    "\n",
    "Specifically, while a NumPy `ndarray` stores just the raster values and has some properties such as the `shape` (number of elements along each axis) and `ndim` (the dimensions of the array) it does not explicitly store any geospatial, temporal, or other geographic metadata. `xarray` solves this problem by reading raster data into an `xarray.DataArray` object with:\n",
    "\n",
    "* `values`: the multidimensional array of raster values\n",
    "* `dims`: a list of names for the dimensions of the array (e.g. instead of axis 0 describing the 0th (row) dimension of an array, that dimension can have a descriptive label such as longitude)\n",
    "* `coordinates`: a `list` of array-like objects that describe the location of an array element along that dimension (e.g. a 1D array of longitude values describing the location on the Earth's surface for each row in the array)\n",
    "* `attrs`: a `dict` of metadata attributes describing the dataset\n",
    "\n",
    "`xarray.DataArray` objects can be stored within a larger container called `xarray.Dataset`. An `xarray.Dataset` can store many `xarray.DataArray` objects that share `dims` and `coordinates`. This is useful if you have different arrays of different `Variables` that correspond to the same locations and time-periods (e.g. you could have a separate array for temperature and precipitation values organised within a single `xarray.Dataset`).\n",
    "\n",
    "![](https://docs.xarray.dev/en/stable/_images/dataset-diagram.png)\n",
    "\n",
    "*Schematic of an xarray.Dataset (source: xarray Getting Started)*\n",
    "\n",
    "**Why is Xarray useful for geospatial data?** \n",
    "\n",
    "* The `dims` and `coordinates` of an `xarray.DataArray` mean we can subset values from an array using latitude, longitude, time, or whatever a coordinate describes; we're not just restricted to subsetting values based on their index location within an array\n",
    "* `xarray.Dataset` objects provide a container to store multidimensional arrays (e.g. many variables and time points) that are common in geography, Earth Sciences, meteorology, and agriculture. For example, multispectral satellite images of the same location over time; arrays of different meteorological variables)\n",
    "* useful functions for reading, analysing and visualising raster or array-like geospatial data that are common across many spatial data science workflows\n",
    "\n",
    "### Data input\n",
    "\n",
    "The `rioxarray` package provides tools for reading and writing raster geospatial data files into `xarray.DataArray` objects.\n",
    "\n",
    "Let's pass the path to a GeoTIFF file of raster data into the `rioxarray` `open_rasterio()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3f117-8721-467d-b552-430d5b1b6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = os.path.join(DATA_PATH, \"chile_predictors.tif\")\n",
    "xds = rxr.open_rasterio(preds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00a5fc-daf7-4a50-9159-1d8ad76ea723",
   "metadata": {},
   "source": [
    "We have used `rioxarray` to read raster data stored in a GeoTIFF file into our program as an `xarray.DataArray` object referenced by the variable `xds`. We can print the `xarray.DataArray` object to inspect its metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cda410-2b10-4ddb-8ee9-675bd5ea27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3401a-7560-42b5-b6f1-ae2945d15b2e",
   "metadata": {},
   "source": [
    "This raster dataset has 11 bands which correspond to the bioclimatic end elevation predictor variables in the training data stored as a `GeoDataFrame` `gdf`.\n",
    "\n",
    "The attributes property of `xds` stores geospatial metadata such as the coordinate reference system (CRS) and the extent of the dataset. We can access this information via the `xds` object's `rio` accessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c507f-b957-44b4-8d46-46e4d063587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CRS:', xds.rio.crs)\n",
    "print('Resolution:', xds.rio.resolution())\n",
    "print('Bounds:', xds.rio.bounds())\n",
    "print('Width:', xds.rio.width)\n",
    "print('Height:', xds.rio.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9605898-298f-4cf3-9706-54eff5551cd8",
   "metadata": {},
   "source": [
    "The raster data values are stored as arrays within the `xarray.DataArray` object and can be accessed via the `values` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27016a4b-57d3-403b-a82b-7c3780cd5e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = xds.values\n",
    "print(f\"the shape of the array is {arr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065843ab-8337-446c-acea-15f846f52505",
   "metadata": {},
   "source": [
    "`xarray.DataArray` objects have a <a href=\"https://docs.xarray.dev/en/stable/generated/xarray.plot.imshow.html\" target=\"_blank\">`plot.imshow()`</a> method that will render array based data as an image. Let's plot band 11, elevation, as a surface. This is the extent of the area we're considering applying our machine learning to, to generate a predicted map of plant species richness. Our task is to assess which locations within this extent we can use our model given the characteristics of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942632a-283c-43c6-bec0-f6d035591232",
   "metadata": {},
   "outputs": [],
   "source": [
    "xds.sel(band=11).plot.imshow(figsize=(3, 9), robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd5c5f-5f55-4f3c-97b8-8aa6d2af79a8",
   "metadata": {},
   "source": [
    "### Compute the dissimilarity index\n",
    "\n",
    "For each location where we have predictor variables we compute the dissimilarity index relative to the training data. \n",
    "\n",
    "Let's extract the predictor variables out of the `GeoDataFrame` and store them as a NumPy `ndarray`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275ce32-fffb-4239-af9f-e6c3a93e6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns not needed for model development\n",
    "gdf_tmp = gdf.drop(columns=[\"PlotObservationID\", \"GIVD_ID\", \"Country\", \"Biome\", \"geometry\"])\n",
    "X = gdf_tmp.drop(columns=[\"Species_richness\"]).to_numpy(copy=True)\n",
    "y = gdf_tmp.loc[:, \"Species_richness\"].to_numpy(copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2179f9-b093-4ede-b7a9-49f3ba416ab5",
   "metadata": {},
   "source": [
    "Next, we'll iterate over each location where we have predictor variables, compute the dissimilarity index relative to the training data, and store the dissimilarity index values in a NumPy `ndarray` `di`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6256b571-fc89-47b8-b6b0-b552401305b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty array to store \n",
    "di_map = np.zeros((xds.shape[1], xds.shape[2]))\n",
    "\n",
    "# compute the average dissimilarity index for the training data\n",
    "# this is a one-time compute operation so do this first and pass\n",
    "# dbar into compute_di() to save compute time\n",
    "dbar = compute_dbar(X, False)\n",
    "\n",
    "# iterate over each location in xds\n",
    "# compute the di between the predictor\n",
    "# values at the location in question and \n",
    "# the training data\n",
    "for r in tqdm(range(0, xds.shape[1])):\n",
    "    for c in range(0, xds.shape[2]):\n",
    "        preds = xds[:, r, c]\n",
    "        if np.any(np.isnan(preds)):\n",
    "            di_map[r, c] = np.nan\n",
    "            continue\n",
    "        di_tmp = compute_di(X, np.expand_dims(preds, 0), dbar, False)[0]\n",
    "        di_map[r, c] = di_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c5993b-8714-418c-8127-d8328bdf5033",
   "metadata": {},
   "source": [
    "Let's plot the dissimilarity index values. This is a map describing how similar predictor values at a location are to values for predictor variables in the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001194a3-0adb-43d3-aa2b-57c410c22753",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,9))\n",
    "plt.imshow(di_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d102a83-24d5-4813-9ca9-077dac8e3fd6",
   "metadata": {},
   "source": [
    "### Dissimilarity index threshold via Random k-fold cross-validation\n",
    "\n",
    "We now have a map of dissimilarity index values for all locations where we have predictor variables. Next, we need to know what level of dissimilarity index values implies that the values of the predictor variables are not represented in the training data. We can estimate this threshold using k-fold cross-validation with the training data and compute the outlier-removed maximum dissimilarity index for all locations in the hold-out folds. Any dissimilarity index value less than this threshold in the predictors dataset is within the area of applicability for the model. \n",
    "\n",
    "Let's compute the dissimilarity index threshold using random k-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1ac0a7-51e1-452a-91f2-387b31ed687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns not used for model training\n",
    "cols_to_drop = [\"PlotObservationID\", \"GIVD_ID\", \"Country\", \"Biome\", \"geometry\"]\n",
    "\n",
    "# Note, we pass in False to the spatial_cv argument\n",
    "di = compute_di_theshold(gdf, 10, None, False, \"Species_richness\", cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e671960-f071-4f5a-b679-669bae7dd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The DI threshold computed with random k-fold cross-validation: {di['di_threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bcca30-486f-4b28-8f13-673d0cc661aa",
   "metadata": {},
   "source": [
    "We can use this threshold to highlight locations outside the area of applicability in the predictors dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b06b0-e031-4a7f-a902-9c80602d8679",
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_aoa = di_map > di[\"di_threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebae0f4-6d32-47e9-ad38-3030de09797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,9))\n",
    "plt.imshow(outside_aoa)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9435a6-e038-4ed1-89f7-7aa6c45ef0ce",
   "metadata": {},
   "source": [
    "### Spatial k-fold cross-validation\n",
    "\n",
    "Next, let's compute the dissimilarity index threshold using spatial k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae61e6e-042e-41c9-b9c0-35733d711de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns not used for model training\n",
    "cols_to_drop = [\"PlotObservationID\", \"GIVD_ID\", \"Country\", \"Biome\", \"geometry\"]\n",
    "\n",
    "# Note, we pass in True to the spatial_cv argument\n",
    "di = compute_di_theshold(gdf, 10, None, True, \"Species_richness\", cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ad53c-d55e-42fb-94f9-ceff60cac170",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The DI threshold computed with spatial k-fold cross-validation: {di['di_threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff2e90-0f25-42da-938e-1824c74836e2",
   "metadata": {},
   "source": [
    "Map the locations outside the area of applicability when the dissimilarity index threshold is computed using spatial k-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8911e18-a848-4fea-8c32-5293dcdf21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_aoa_sp_cv = di_map > di[\"di_threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379966eb-6782-4aeb-a0f0-1410b9df9e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,9))\n",
    "plt.imshow(outside_aoa_sp_cv)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6685135-3694-4b87-85d6-8af66b942798",
   "metadata": {},
   "source": [
    "Depending on whether we use random or spatial k-fold cross-validation we get very different estimates of the dissimilarity index threshold and the area of applicability for the machine learning model. \n",
    "\n",
    "#### Activity!\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Why is the dissimilarity index threshold higher and the area of applicability lower when estimated using spatial k-fold cross-validation?</summary>\n",
    "    \n",
    "Spatial k-fold cross-validation splits the training data into spatial clusters and holds out each cluster in-turn. This strategy to splitting the data creates larger distances between training and test folds which mimics the situation of evaluating a model when deployed far away from the locations of the training data. Thus, assuming the predictor values in the training data are spatially correlated, it is likely that there will be higher dissimilarity values between locations in the training and test folds. This, in turn, will return a higher dissimilarity index threshold.\n",
    "\n",
    "If the training data has a degree of spatial clustering, creating hold out folds using random k-fold cross-validation will mean that examples in a hold-out fold are likely close in space to examples in a training fold. Thus, the dissimilarity index values between training and hold-out folds will be smaller and the dissimilarity index threshold will also be smaller. This means we're only able to assess the model's application close to locations in the training dataset. Therefore, the area of application will be much  smaller. \n",
    "\n",
    "The trade-off is while using spatial k-fold cross-validation will result in a larger area of application for the machine learning model there will be a corresponding larger model error estimate.\n",
    "\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "Let's explore the relationship between model error and the dissimilairty index further. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccfca3-f239-41eb-8a86-859b5afa0dba",
   "metadata": {},
   "source": [
    "## Relationship between dissimilarity index and model error\n",
    "\n",
    "We would expect model error to increase as the dissimilarity index increases reflecting a situations where the predictor data is less similar to values in the training dataset. <a href=\"https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13650\" target=\"_blank\">Meyer and Pebesma (2021)</a> outline an approach for exploring the relationship between the dissimilarity index and model error:\n",
    "\n",
    "1. Iterate over a list of $n$ folds:\n",
    "    * Implement k-fold cross-validation with $n_{k}$ folds.\n",
    "    * Compute the dissimilarity index for all examples in the hold-out folds.\n",
    "    * Compute the error for each prediction in the hold-out folds. \n",
    "2. Sort the dissimilarity index values in ascending order.\n",
    "3. Compute the RMSE of predictions on the hold-out folds in moving window of size 10.\n",
    "4. Plot dissimilarity index values against RMSE computed within moving windows.\n",
    "\n",
    "Let's quickly create a helper function the implements these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb5116-8ace-4d86-bb27-1f71b7867902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def di_model_error(gdf, spatial_cv, target, cols_to_drop):\n",
    "    \"\"\"Plot the relationship between dissimilarity index and model error\n",
    "    \n",
    "    Args:\n",
    "        gdf (GeoDataFrame): The spatial dataset (GeoDataFrame) containing features and the target variable.\n",
    "        spatial_cv (bool): If `True`, use k-fold spatial cross-validation. Otherwise, use k-fold random cross-validation.\n",
    "        target_column (str): The column name for the target variable.\n",
    "        cols_to_drop (list): Columns to drop that should not be included as predictors.\n",
    "\n",
    "    Returns:\n",
    "        scatter plot visualising the relationship between the dissimilarity index and model error. \n",
    "    \"\"\"\n",
    "    # set up a model\n",
    "    model = MLPRegressor(hidden_layer_sizes=(50,), random_state=4, solver=\"sgd\", max_iter=500)\n",
    "        \n",
    "    # loop over k-fold validation with different numbers of folds\n",
    "    di_preds = []\n",
    "    for f in [3, 5, 10, 20, 30]:\n",
    "        di_preds.append(compute_di_theshold(gdf, f, model, spatial_cv, target, cols_to_drop))\n",
    "\n",
    "    # create a DataFrame of DI values and squared errors\n",
    "    di_preds_df = pd.DataFrame()\n",
    "    \n",
    "    for i in di_preds:\n",
    "        tmp_df = pd.DataFrame({\n",
    "            \"DI\": i[\"di\"],\n",
    "            \"squared_error\": np.abs(i[\"errors\"]) * np.abs(i[\"errors\"])\n",
    "        })\n",
    "    \n",
    "        di_preds_df = pd.concat([di_preds_df, tmp_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    # sort the DataFrame by DI values\n",
    "    di_preds_sorted = di_preds_df.sort_values(by=[\"DI\"], ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    # compute the RMSE within moving window over ascending DI values\n",
    "    window_size = 10\n",
    "    \n",
    "    # Calculate rolling RMSE\n",
    "    di_preds_sorted.loc[:, \"RMSE\"] = (\n",
    "        di_preds_sorted.loc[:, \"squared_error\"]\n",
    "        .rolling(window=window_size)\n",
    "        .apply(lambda x: np.sqrt(np.mean(x)), raw=True)\n",
    "    )\n",
    "\n",
    "    sns.regplot(data=di_preds_sorted, x=\"DI\", y=\"RMSE\", lowess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197b259-0aec-4827-92f8-b6d51519c8c7",
   "metadata": {},
   "source": [
    "Now we can plot the relationship between the dissimilarity index and cross-validation error using spatial k-fold cross-validation. We can see there is an increasing trend indicating that as the dissimilarity between prediction locations and the training data increases the model error also increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d95e5-1d99-442c-ad20-11edc81e7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns not used for model training\n",
    "cols_to_drop = [\"PlotObservationID\", \"GIVD_ID\", \"Country\", \"Biome\", \"geometry\"]\n",
    "\n",
    "# set spatial_cv to True for spatial k-fold cross-validation\n",
    "di_model_error(gdf, True, \"Species_richness\", cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4e190-a507-42a4-b863-5d42ab63c523",
   "metadata": {},
   "source": [
    "If we use random k-fold cross-validation instead of spatial k-fold cross-validation you will note the relationship between the dissimilarity index and model perror persists. However, we have a smaller range of dissimilarity index values and lower model error estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd3cbe-938d-4f87-9f76-db84e029b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns not used for model training\n",
    "cols_to_drop = [\"PlotObservationID\", \"GIVD_ID\", \"Country\", \"Biome\", \"geometry\"]\n",
    "\n",
    "# set spatial_cv to False for random k-fold cross-validation\n",
    "di_model_error(gdf, False, \"Species_richness\", cols_to_drop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
