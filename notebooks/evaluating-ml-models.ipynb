{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd2325f-22d7-47c3-add9-b5437e7bca57",
   "metadata": {},
   "source": [
    "# Evaluating geospatial ML models\n",
    "\n",
    "## Estimating model error\n",
    "\n",
    "After training a machine learning model, it is important to evaluate the model's performance and assess how suitable the model is for a particular task or application. \n",
    "\n",
    "The *training error* is the difference between the model's prediction and known values for the training examples. This gives us a biased estimate of the model's performance as, during training, the model has been optimised to map input data to target values in training dataset. \n",
    "\n",
    "What is more relevant is the model's <a href=\"https://d2l.ai/chapter_linear-regression/generalization.html#training-error-and-generalization-error\" target=\"_blank\">*generalisation error*</a> which indicates how well the model would perform if it was applied to the target population. It is oftem impossible to compute the *generalisation error* as it is not feasible obtain target values for the entire target population to compare with model predictions. Therefore, the *generalisation error* is estimated using the test split, a sample of data withheld from the model during training. But, note, this is an *estimate* of the generalisation error, it is the *test set error*, and you should be aware of the characteristics and limits of your test set (i.e. what sampling procedure was used to generate the test set? how representative of the target population is the test set? is there any measurement error in the test set?). \n",
    "\n",
    "Here, our focus is on geospatial data. We're developing machine learning models to predict a geospatial outcome that can be mapped. Our model evaluation is trying to assess how well our predicted map of the target variable matches actual conditions on the ground. We can think of the target population as an area of application for our model; we're seeking to evaluate how well our model works at generating spatial predictions within this extent.\n",
    "\n",
    "## Performance metrics\n",
    "\n",
    "### Classification tasks\n",
    "\n",
    "To estimate the model's error we compute a range of performance metrics. For classification tasks, <a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.accuracy_score.html\" target=\"_blank\">accuracy</a> is a common metric. Accuracy is the percentage of examples in the test set the model correctly labelled. \n",
    "\n",
    "### Regression tasks\n",
    "\n",
    "For regression tasks, <a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.mean_squared_error.html\" target=\"_blank\">mean squared error</a> (MSE), root mean squared error (RMSE), <a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.mean_absolute_error.html\" target=\"_blank\">mean absolute error</a> (MAE), the root mean squared error (RMSE), and <a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.r2_score.html\" target=\"_blank\">$R^2$</a> are common metrics. \n",
    "\n",
    "The MSE measures the average of squared distances between the model predicted and true outcome values in the test set. As it measures the squared distance it is more sensitive to cases when the model error is large. \n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \n",
    "$$\n",
    "\n",
    "The RMSE is the square root of the MSE which converts the error into units of the target variable. \n",
    "\n",
    "The MAE is similar but it measures the average of absolute distances between the model predicted and true outcome values in the test set; it is less sensitive to cases when the model error is large.\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "The $R^2$ (also known as the coefficient of determination) measures the amount of variability in the true outcome values that is explained by the model predicted outcomes. The closer the $R^2$ value is to one, the more variability in the test set's true outcomes is captured by the model's predictions. The $R^2$ is a measure of how well the model's predictions fit the true outcomes. \n",
    "\n",
    "$$\n",
    "\\text{R}^2 = 1- \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2} \n",
    "$$\n",
    "\n",
    "$\\bar{y}$ is the mean of true outcomes in the test set and $n$ is the number of examples in the test set. \n",
    "\n",
    "## Evaluation strategies\n",
    "\n",
    "### The ideal test set\n",
    "\n",
    "If we know the target population where our model will deployed, we can generate a probability sample from this population to use as a test set (e.g. via a random sample or stratified random sample). A probability sample implies every example has a known probability of being included in the sample. As this is a probability sample, the error computed using this test set will be an unbiased estimate of the true population error (i.e. if we took many repeat test set samples and estimated the error, the average error across all sampling-error estimation iterations will be the true population error).  \n",
    "\n",
    "### Train-validation-test splits\n",
    "\n",
    "In many cases, it is not possible to generate a probability sample test set from the target population. This could be because it is too costly to collect a probability sample of sufficient size (e.g. fieldwork in remote locations) or because we did not have control over the generation of the dataset available to us (e.g. meteorological data from weather stations with fixed locations in space). In such cases, we can split the dataset available to us into training, test and optionally validation datasets. The validation split can used in model training and development to refine model hyperparameters. This strategy to splitting the dataset is viable when there are sufficient examples to allocate into training and test splits. However, unless the initial dataset was generated as a probability sample, there is no way of ensuring that test error estimate will be unbiased or reflective of the true model error for the target population. \n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "A limitation of train-test splits is that not all of the dataset is used for model training. This is particularly problematic when we have a small dataset and we want to maximise the amount of data our model can learn from. One strategy that is deployed to maximise data available for model training and to provide an assessment of model performance is k-fold cross-validation. In k-fold cross-validation there is not a single test set. Instead, the dataset is randomly split into $k$ folds. For example, if $k=5$ the dataset would be randomly split into 5 groups. Then, in turn, each fold is held out as a test set and the model is trained using data from the remaining four folds. Each fold takes a turn at being the test set. The model performance can be summarised using the average of the performance metrics generated using each fold. This means the model's performance is less susceptible to being influenced by a chance split of the initial dataset into training and test splits. It also means we can use the whole dataset to train the model and evaluate its performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe2a98-478a-4e12-b862-0644e0fa550a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5ead6-4698-43e0-b506-6b349c3a3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "if \"data-geoml\" not in os.listdir(os.getcwd()):\n",
    "    subprocess.run('wget \"https://github.com/envt-5566/geo-ml/raw/main/data/data-geoml.zip\"', shell=True, capture_output=True, text=True)\n",
    "    subprocess.run('unzip \"data-geoml.zip\"', shell=True, capture_output=True, text=True)\n",
    "    if \"data-geoml.zip\" not in os.listdir(os.getcwd()):\n",
    "        print(\"Has a directory called data-geoml been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n",
    "    else:\n",
    "        print(\"Data download OK\")\n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10ab7f-2e38-45d2-9d92-1cf775c37571",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6212e-e7d9-4503-bad6-929dd16e9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install mapclassify\n",
    "    !pip install contextily\n",
    "    !pip install pysal\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# spatial analysis libraries\n",
    "import pysal\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import contextily as cx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# models\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18245a9d-c1ed-4bf3-bc44-ad34ea838358",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "Read in the plant species richness dataset for South America from the <a href=\"https://onlinelibrary.wiley.com/doi/10.1111/geb.13346\" target=\"_blank\">sPlotOpen database</a>. This is a spatial point dataset where each dataset has counts of species richness and a series of predictor variables representing bioclimatic and topographic features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931acdb-c344-4ad0-8529-108d7910746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(os.path.join(DATA_PATH, \"plant_species_south_america.gpkg\"))\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b410e-1913-49d9-aedc-e62304f1bfeb",
   "metadata": {},
   "source": [
    "## Non-spatial model evaluation\n",
    "\n",
    "First, we will demonstrate non-spatial model evaluation using a random train-test split and random k-fold cross-validation. \n",
    "\n",
    "### Train-test split\n",
    "\n",
    "#### Data pre-processing\n",
    "\n",
    "Create a random test split of 30% of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2a55c-083d-4a36-969c-a89d505e9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns not needed for model development\n",
    "gdf_tt_splits = gdf.drop(columns=[\"PlotObservationID\", \"GIVD_ID\", \"Country\", \"Biome\", \"geometry\"])\n",
    "X = gdf_tt_splits.drop(columns=[\"Species_richness\"])\n",
    "y = gdf_tt_splits.loc[:, \"Species_richness\"]\n",
    "\n",
    "# set aside 30% of the data as a test split\n",
    "# set the random state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc49aae-cc5b-4fb0-a998-95a734e92576",
   "metadata": {},
   "source": [
    "Standardise the input predictor data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb50e55-c490-4b5d-a6ea-a51fb3ad494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de59ab-974c-45a0-8d30-62581c6f58d9",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "\n",
    "Train a Multi-layer perceptron model with a hidden layer of 50 units as a regression task to predict species richness at a point based on bioclimatic variables and elevation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779001a1-2741-415a-9efa-79b0d805f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(50,), random_state=4, solver=\"sgd\", max_iter=500).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a30c2-7501-45b3-b7ce-969f52feccf1",
   "metadata": {},
   "source": [
    "#### Model evaluation\n",
    "\n",
    "First, let's compute the training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2049f5-4c54-470e-8166-af8cab53b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = regr.predict(X_train_scaled)\n",
    "train_split_mse = mean_squared_error(y_train, y_train_preds)\n",
    "print(f\"The MSE on the train split is: {round(train_split_mse, 2)}\")\n",
    "print(f\"The RMSE on the train split is: {round(math.sqrt(train_split_mse), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691deba3-6e9d-4059-94eb-50bf72e0c402",
   "metadata": {},
   "source": [
    "Next, let's compute the test set error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013bc6f4-d79e-4111-a00c-7f3f8df0c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = regr.predict(X_test_scaled)\n",
    "test_split_mse = mean_squared_error(y_test, y_test_preds)\n",
    "print(f\"The MSE on the test split is: {round(test_split_mse, 2)}\")\n",
    "print(f\"The RMSE on the test split is: {round(math.sqrt(test_split_mse), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1a61e-634e-4af2-8637-891bb72fd4e7",
   "metadata": {},
   "source": [
    "#### Activity!\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Why is the test set error higher than the training error?</summary>\n",
    "    \n",
    "During model training, the model will have seen the training data and learnt parameter values are that optimised to map input training data to target values. Thus, the model's performance on the training split will be biased and overoptimistic relative to how the model would perform on new unseen data.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff8256-1ffd-44a1-9f6b-9960915aea2f",
   "metadata": {},
   "source": [
    "#### Activity!\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Out target area of application is South America. We took a random split of the dataset to use as the test split, does this mean our test error is an unbiased estimate of the model's generalisation error across all of South America?</summary>\n",
    "    \n",
    "No. While we took a random sample from the dataset available to us to use as the test split, the initial dataset we started with is not a probability sample that's representative of our target area of application. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88acf1bc-3fbe-4f38-93e7-cbfae8711ef9",
   "metadata": {},
   "source": [
    "### K-fold cross-validation\n",
    "\n",
    "`scikit-learn` has convenience functions to help with k-fold cross-validation. However, here we will create our own k-fold cross-validation function. This will make the steps involved in implementing k-fold cross-validation explicit:\n",
    "\n",
    "1. Randomly shuffle a `GeoDataFrame` where each row corresponds to an example in our dataset.\n",
    "2. Allocate each example in the shuffled `GeoDataFrame` to a fold based on its row index location (the random shuffle makes this a random allocation of examples to folds).\n",
    "3. Loop over fold ids:\n",
    "    * Hold out the examples with the current fold ids as the test fold.\n",
    "    * Compute standardisation statistics on the remaining folds (per-variable mean and standard deviation).\n",
    "    * Standardise the train and test fold input predictors.\n",
    "    * Train a model using the training folds.\n",
    "    * Evaluate model using the test folds.\n",
    "    * Store the metric score in a list.\n",
    "4. Return the list of metrics scores when all folds have had a turn as the test fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b1b4d-bd4b-4f80-b1a8-b48d7897371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cross_validation(gdf, n_folds, model, metric, target_column, cols_to_drop):\n",
    "    \"\"\"Function to perform k-fold cross-validation.\n",
    "    Args:\n",
    "        gdf (GeoDataFrame): The spatial dataset (GeoDataFrame) containing features and the target variable.\n",
    "        n_folds (int): The number of folds to use for cross-validation.\n",
    "        model: The machine learning model to train and evaluate.\n",
    "        metric: The evaluation metric function to use for computing errors on the test fold.\n",
    "        target_column (str): The column name for the target variable.\n",
    "        cols_to_drop (list): Columns to drop that should not be included as predictors.\n",
    "\n",
    "    Returns:\n",
    "        list, GeoDataFrame: A list containing the error metrics computed on each test fold during cross-validation\n",
    "        and a GeoDataFrame with random folds appended as a column for plotting. \n",
    "    \"\"\"\n",
    "    gdf_tmp = gdf.copy()\n",
    "\n",
    "    # split the data into n_folds\n",
    "    # assign a fold id column to each row\n",
    "    # shuffle the GeoDataFrame\n",
    "    gdf_shuffled = gdf_tmp.sample(frac=1, random_state=123)\n",
    "\n",
    "    # assign a fold id using the modulo operation\n",
    "    gdf_shuffled.loc[:, \"fold\"] = np.arange(len(gdf_shuffled)) % n_folds\n",
    "\n",
    "    # store shuffled gdf to return for plotting\n",
    "    gdf_folds = gdf_shuffled.copy()\n",
    "    \n",
    "    # create an empty list to store metrics computed on test fold\n",
    "    errors = []   \n",
    "    \n",
    "    gdf_shuffled = gdf_shuffled.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # loop of folds, train a model, and compute error\n",
    "    for k in range(0, n_folds):\n",
    "        # subset out test fold\n",
    "        gdf_test_fold = gdf_shuffled.loc[gdf_shuffled[\"fold\"] == k, :]\n",
    "        \n",
    "        # subset out training folds\n",
    "        # != is the not equals to operator\n",
    "        gdf_train_fold = gdf_shuffled.loc[gdf_shuffled[\"fold\"] != k, :]\n",
    "\n",
    "        # subset training folds into predictors and targets arrays\n",
    "        X_train = gdf_train_fold.drop(columns=[target_column, \"fold\"])\n",
    "        y_train = gdf_train_fold.loc[:, target_column]\n",
    "\n",
    "        # subset test fold into predictors and targets arrays\n",
    "        X_test = gdf_test_fold.drop(columns=[target_column, \"fold\"])\n",
    "        y_test = gdf_test_fold.loc[:, target_column]\n",
    "\n",
    "        # standardise data\n",
    "        tmp_scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = tmp_scaler.transform(X_train)\n",
    "        X_test_scaled = tmp_scaler.transform(X_test)\n",
    "\n",
    "        # train model \n",
    "        m = model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # compute error using the test fold \n",
    "        y_test_preds = m.predict(X_test_scaled)\n",
    "\n",
    "        # append errors to list\n",
    "        errors.append(metric(y_test, y_test_preds))\n",
    "\n",
    "    # return list error metrics for test folds\n",
    "    return errors, gdf_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246319be-a5d9-4b8b-a082-ffd2e6c0f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to drop that are not predictors\n",
    "cols_to_drop = [\"PlotObservationID\", \"GIVD_ID\", \"Country\", \"Biome\", \"geometry\"]\n",
    "\n",
    "# set up a model\n",
    "model = MLPRegressor(hidden_layer_sizes=(50,), random_state=4, solver=\"sgd\", max_iter=500)\n",
    "\n",
    "# run k-fold cross-validation with 5 folds\n",
    "random_cv_errors, gdf_folds = random_cross_validation(gdf, 5, model, mean_squared_error, \"Species_richness\", cols_to_drop)\n",
    "\n",
    "# print cv_errors\n",
    "for i, e in enumerate(random_cv_errors):\n",
    "    print(f\"The MSE for fold {i} is {round(e, 2)}\")\n",
    "    print(f\"The RMSE for fold {i} is {round(math.sqrt(e), 2)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9950c3fe-702f-4336-b0db-0fcf57350b85",
   "metadata": {},
   "source": [
    "We can visualise the points fold membership to check we have a random allocation of points to folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0725fa-6084-4a9b-ad31-2d1e33d033d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot folds on a map\n",
    "# use a categorical colourmap to visualise folds\n",
    "ax = gdf_folds.plot(column=\"fold\", categorical=True, cmap=\"Set1\", legend=True)\n",
    "\n",
    "# Add a basemap\n",
    "cx.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.Positron,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4164b4d7",
   "metadata": {},
   "source": [
    "## Considerations for geospatial data\n",
    "\n",
    "In geospatial machine learning we are developing models to generate predictions that can be mapped. To evaluate a map, the desired test dataset is a probability sample of locations, drawn independently from the training data, that is representative of the target area. However, in many cases a test dataset with these characteristics does not exist and it is not possible to collect one. \n",
    "\n",
    "If the dataset is spatially auto-correlated and clustered, then spatially naive random subsetting of the dataset into training and test sets (either via a train-test splits or k-fold cross validation) will result in test data that is i) not independent of the training data, and ii) not representative of unsampled locations within the target area. This will result in  underestimating the model's error when the model is deployed in locations within the target area not covered in the training data. A condition for the generating train-test sets or k-fold cross validation via spatially naive random splitting is the the training and test sets are independent, this is not the case with spatially autocorrelated and clustered data. \n",
    "\n",
    "Various strategies have been proposed to estimate a model's performance across a target area in the presence of spatially auto-correlated and clustered data. These include:\n",
    " \n",
    "* **Spatially buffered validation**: A buffer distance is used to separate training and test splits. This is to reduce the spatial auto-correlation between training and test examples and provide an estimate of model performance in areas not represented in the training dataset.\n",
    "* **Spatial k-fold cross validation**: Instead of randomly allocating examples to folds, the dataset is spatially clustered and each cluster forms a fold in k-fold crosss-validation. As each fold is spatially separate from the remaining folds, this gives an indication of how well the model performs when deployed in areas not represented in the training dataset.\n",
    "* **Nearest neighbour distance matching**: A leave-one-out variant of cross-validation where the examples held out for testing are selected to resemble the distance between the training dataset and the prediction area.\n",
    "* **k-fold nearest neighbour distance matching**: A variant of k-fold cross-validation where the folds held out for testing are selected to resemble the distance between the training dataset and the prediction area.\n",
    "\n",
    "The first task when evaluating geospatial machine learning model is to assess how spatially autocorrelated and clustered the dataset is. This will give an early indication as to whether estimates of model performance based on this dataset will underestaimte the model's error when applied to the target area. \n",
    "\n",
    "We can visualise the distribition of points across the target area by plotting each point on a scatter plot with longitude and latitude mapped to the x- and y-axes and render histograms of point densities along each axis (see <a href=\"https://geographicdata.science/book/notebooks/08_point_pattern_analysis.html#introduction\" target=\"_blank\">Rey et al. (2020)</a> for more information on point pattern analysis). \n",
    "\n",
    "A visual inspection of the data illustrates that our data is not evenly distributed across South America but is clustered along the North- and South-Western coastline with other clusters on the South-East coast of Brazil, on the Brazil-Bolivia border and Northern-Central Argentina. Further, if the data was evenly distributed across the target area the histograms on the x- and y-axes would have similar heights; as they do not it is an indication that the data has some clustered patterns. \n",
    "\n",
    "This is an early indication this dataset is not representative of conditions across all of South America and evaluating a model trained and tested using this dataset will give an overly optimistic assessment of the model's performance across the target area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b724601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering / spatial correlation in the data\n",
    "gdf_plotting = gdf.copy()\n",
    "\n",
    "# add x and y columns\n",
    "gdf_plotting.loc[:, \"x\"] = gdf_plotting.loc[:, \"geometry\"].x\n",
    "gdf_plotting.loc[:, \"y\"] = gdf_plotting.loc[:, \"geometry\"].y\n",
    "\n",
    "# Generate  xy-scatter plot\n",
    "xy_scatter = sns.jointplot(\n",
    "    x=\"x\", \n",
    "    y=\"y\", \n",
    "    data=gdf_plotting, \n",
    "    s=2,\n",
    "    color=\"magenta\",\n",
    ")\n",
    "\n",
    "# Add a basemap\n",
    "cx.add_basemap(\n",
    "    xy_scatter.ax_joint,\n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.Positron,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89261d-8e31-49e3-9662-bcc189602ee9",
   "metadata": {},
   "source": [
    "### Spatial k-fold cross-validation\n",
    "\n",
    "We can use spatial k-fold cross-validation to assess how well the model would perform if it was used to generate predictions in locations within the target area where there are no training data. Again, we will write a custom function to perform spatial k-fold cross-validation to make the steps in the process explicit:\n",
    "\n",
    "1. Create an array with the number of rows (0-axis) corresponding to the number of points in our dataset and two columns (1-axis). The values of the array are x and y values corresponding to longitude and latitude.\n",
    "2. Use a K Means clustering algorithm to cluster the dataset into $k$ groups with similar longitude and latitude values. These groups become our folds.\n",
    "3. Loop over fold ids:\n",
    "    * Hold out the examples with the current fold ids as the test fold.\n",
    "    * Compute standardisation statistics on the remaining folds (per-variable mean and standard deviation).\n",
    "    * Standardise the train and test fold input predictors.\n",
    "    * Train a model using the training folds.\n",
    "    * Evaluate model using the test folds.\n",
    "    * Store the metric score in a list.\n",
    "4. Return the list of metrics scores when all folds have had a turn as the test fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec603c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_cross_validation(gdf, n_folds, model, metric, target_column, cols_to_drop):\n",
    "    \"\"\"Function to perform spatial k-fold cross-validation.\n",
    "    Args:\n",
    "        gdf (GeoDataFrame): The spatial dataset (GeoDataFrame) containing features and the target variable.\n",
    "        n_folds (int): The number of spatial folds to use for cross-validation.\n",
    "        model: The machine learning model to train and evaluate.\n",
    "        metric: The evaluation metric function to use for computing errors on the test fold.\n",
    "        target_column (str): The column name for the target variable.\n",
    "        cols_to_drop (list): Columns to drop that should not be included as predictors. \n",
    "\n",
    "    Returns:\n",
    "        list, GeoDataFrame: A list containing the error metrics computed on each test fold during cross-validation \n",
    "        and GeoDataFrame with spatial folds for plotting. \n",
    "    \"\"\"\n",
    "    gdf_tmp = gdf.copy()\n",
    "\n",
    "    # split the data into n_folds spatially\n",
    "    # assign a fold id column to each row\n",
    "\n",
    "    # add x and y columns\n",
    "    gdf_tmp.loc[:, \"x\"] = gdf_tmp.loc[:, \"geometry\"].x\n",
    "    gdf_tmp.loc[:, \"y\"] = gdf_tmp.loc[:, \"geometry\"].y\n",
    "\n",
    "    # create an array of x and y values\n",
    "    X = gdf_tmp.loc[:, [\"x\", \"y\"]].copy()\n",
    "\n",
    "    # create a KMeans clusterer\n",
    "    km = KMeans(\n",
    "        n_clusters=n_folds,\n",
    "        init=\"random\",\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        tol=1e-04,\n",
    "        random_state=123,\n",
    "    )\n",
    "    spatial_folds = km.fit_predict(X)\n",
    "    gdf_tmp.loc[:, \"fold\"] = spatial_folds\n",
    "\n",
    "    # keep a copy of GeoDataFrame with folds for plotting\n",
    "    gdf_spatial_folds = gdf_tmp.copy()\n",
    "    \n",
    "    # drop columns that should not be included as predictors\n",
    "    gdf_tmp = gdf_tmp.drop(columns=cols_to_drop)\n",
    "\n",
    "    # create an empty list to store metrics computed on test fold\n",
    "    errors = []   \n",
    "    \n",
    "    # loop of folds, train a model, and compute error\n",
    "    for k in range(0, n_folds):\n",
    "        # subset out test fold\n",
    "        gdf_test_fold = gdf_tmp.loc[gdf_tmp[\"fold\"] == k, :]\n",
    "        \n",
    "        # subset out training folds\n",
    "        # != is the not equals to operator\n",
    "        gdf_train_fold = gdf_tmp.loc[gdf_tmp[\"fold\"] != k, :]\n",
    "\n",
    "        # subset training folds into predictors and targets arrays\n",
    "        X_train = gdf_train_fold.drop(columns=[target_column, \"fold\"])\n",
    "        y_train = gdf_train_fold.loc[:, target_column]\n",
    "\n",
    "        # subset test fold into predictors and targets arrays\n",
    "        X_test = gdf_test_fold.drop(columns=[target_column, \"fold\"])\n",
    "        y_test = gdf_test_fold.loc[:, target_column]\n",
    "\n",
    "        # standardise data\n",
    "        tmp_scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = tmp_scaler.transform(X_train)\n",
    "        X_test_scaled = tmp_scaler.transform(X_test)\n",
    "\n",
    "        # train model \n",
    "        m = model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # compute error using the test fold \n",
    "        y_test_preds = m.predict(X_test_scaled)\n",
    "\n",
    "        # append errors to list\n",
    "        errors.append(metric(y_test, y_test_preds))\n",
    "\n",
    "    # return list error metrics for test folds\n",
    "    return errors, gdf_spatial_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop these columns when running spatial cv as they should not be included as predictors\n",
    "cols_to_drop = [\"PlotObservationID\", \"GIVD_ID\", \"Country\", \"Biome\", \"geometry\", \"x\", \"y\"]\n",
    "\n",
    "# set up a model\n",
    "model = MLPRegressor(hidden_layer_sizes=(50,), random_state=4, solver=\"sgd\", max_iter=500)\n",
    "\n",
    "# run k-fold cross-validation with 5 folds\n",
    "spatial_cv_errors, gdf_spatial_folds = spatial_cross_validation(gdf, 5, model, mean_squared_error, \"Species_richness\", cols_to_drop)\n",
    "\n",
    "# print spatial cv_errors\n",
    "for i, e in enumerate(spatial_cv_errors):\n",
    "    print(f\"The MSE for fold {i} is {round(e, 2)}\")\n",
    "    print(f\"The RMSE for fold {i} is {round(math.sqrt(e), 2)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc947a-b7db-4480-8d34-a42b207c6207",
   "metadata": {},
   "source": [
    "Let's quickly visualise the spatial folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d6707-7084-4cd1-a769-04b233059b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot spatial folds on a map\n",
    "# use a categorical colourmap to visualise folds\n",
    "ax = gdf_spatial_folds.plot(column=\"fold\", categorical=True, cmap=\"Set1\", legend=True)\n",
    "\n",
    "# Add a basemap\n",
    "cx.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.Positron,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18849472-dd7c-4851-9336-1b372e999585",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "For quick viewing, let's print out the error for the random test split, random k-fold cross-validation, and spatial k-fold cross-validation. Let's also plot a histogram of the target variable to get some perspective on the magnitude of the errors relative to the distribution of target specieis richness values in the dataset. \n",
    "\n",
    "We can see that the RMSE for the test split and for hold-out folds in random k-fold cross-validation is between 20 and 30. This error is roughly equivalent to a half to two-thirds of a standard deviation of the sepcies richness values in the dataset. However, we can see the spatial k-fold cross-validation error is between 50 and 63 for four out of the five folds. This is more than one standard deviation of the species richness values. Thus, when we account for the spatial clustering structure of the dataset and assess how well the model performs in areas with no training data the error increases. If we relied solely on random test splits or random k-fold cross-validation to judge the performance of our model, we would be overly optimistic about how well it would perform in new locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ecb889-32f8-4000-8aff-aa0471690e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random test split error\")\n",
    "print(f\"The RMSE on the test split is: {round(math.sqrt(test_split_mse), 2)}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Random k-fold cross-validation\")\n",
    "for i, e in enumerate(random_cv_errors):\n",
    "    print(f\"The RMSE for fold {i} is {round(math.sqrt(e), 2)}\")\n",
    "    print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Spatial k-fold cross-validation\")\n",
    "for i, e in enumerate(spatial_cv_errors):\n",
    "    print(f\"The RMSE for fold {i} is {round(math.sqrt(e), 2)}\")\n",
    "    print(\"\")\n",
    "\n",
    "# summary stats and histogram of species richness values in the dataset\n",
    "print(\"Summary statistics for species richness variable\")\n",
    "print(gdf[\"Species_richness\"].describe())\n",
    "print(\"\")\n",
    "print(\"Histogram of species richness values\")\n",
    "sns.displot(data=gdf, x=\"Species_richness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b484484",
   "metadata": {},
   "source": [
    "## Self guided activity! Spatial cross-validation assessment of land cover classification\n",
    "\n",
    "Using the Fiji land cover classification dataset, can you assess a classification model's accuracy using spatial cross-validation?\n",
    "\n",
    "#### Activity!\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Can you read the Fiji land cover classification data stored in the GeoJSON file <code>fiji_ba_lulc_s2.geojson</code> into a <code>GeoDataFrame</code>?</summary>\n",
    "    \n",
    "```\n",
    "gdf_lc = gpd.read_file(os.path.join(DATA_PATH, \"fiji_ba_lulc_s2.geojson\"))\n",
    "gdf_lc.explore(column=\"class\", categorical=True, cmap=\"Set1\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ca65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4746c",
   "metadata": {},
   "source": [
    "#### Activity!\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Can you explore the level of spatial clustering of the Fiji land cover classification dataset?</summary>\n",
    "    \n",
    "```\n",
    "# Evaluate clustering / spatial correlation in the data\n",
    "gdf_lc_plotting = gdf_lc.copy()\n",
    "\n",
    "# add x and y columns\n",
    "gdf_lc_plotting.loc[:, \"x\"] = gdf_lc_plotting.loc[:, \"geometry\"].x\n",
    "gdf_lc_plotting.loc[:, \"y\"] = gdf_lc_plotting.loc[:, \"geometry\"].y\n",
    "\n",
    "# Generate  xy-scatter plot\n",
    "xy_scatter = sns.jointplot(\n",
    "    x=\"x\", \n",
    "    y=\"y\", \n",
    "    data=gdf_lc_plotting, \n",
    "    s=2,\n",
    "    color=\"magenta\",\n",
    ")\n",
    "\n",
    "# Add a basemap\n",
    "cx.add_basemap(\n",
    "    xy_scatter.ax_joint,\n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.Positron,\n",
    ")\n",
    "```\n",
    "\n",
    "You should see there is a far more even distribution of points across the Ba region; this is visible on the map and the adjoining histograms.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bac627",
   "metadata": {},
   "source": [
    "#### Activity!\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Can you adapt the random and spatial cross-validation examples above to assess the classifier's performance with the Fiji land cover classification dataset? Use only one hidden layer with 10 units for the model. You can find the documentation for the <code>MLPClassifier</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\", target=\"_blank\">here</a>.</strong></summary>\n",
    "    \n",
    "```\n",
    "cols_to_drop = [\"year\", \"geometry\"]\n",
    "\n",
    "# set the metric to accuracy_score as this is a classification task\n",
    "# set the target variable to \"class\"\n",
    "model = MLPClassifier(hidden_layer_sizes=(10,), random_state=4, solver=\"sgd\", max_iter=500)\n",
    "random_lc_cv_errors, gdf_folds = random_cross_validation(gdf_lc, 5, model, accuracy_score, \"class\", cols_to_drop)\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(10,), random_state=4, solver=\"sgd\", max_iter=500)\n",
    "spatial_lc_cv_errors, gdf_spatial_folds = spatial_cross_validation(gdf_lc, 5, model, accuracy_score, \"class\", cols_to_drop)\n",
    "\n",
    "print(\"Random k-fold cross-validation:\")\n",
    "for i, e in enumerate(random_lc_cv_errors):\n",
    "    print(f\"The accuracy for fold {i} is {round(e, 2)}\")\n",
    "    print(f\"\")\n",
    "\n",
    "print(\"Spatial k-fold cross-validation:\")\n",
    "for i, e in enumerate(spatial_lc_cv_errors):\n",
    "    print(f\"The accuracy for fold {i} is {round(e, 2)}\")\n",
    "    print(f\"\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b70504",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a9c93",
   "metadata": {},
   "source": [
    "## Per-class performance metrics\n",
    "\n",
    "Thus far we have generated a single performance metric for our model. However, for multi-class classification tasks, like the land cover mapping example, it is important to assess how well the model performs for individual classes. \n",
    "\n",
    "#### Activity!\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Why is it important to consider class-level performance metrics when assessing model performance?</summary>\n",
    "    \n",
    "If our dataset is imbalanced (e.g. we have a lot of examples of one class and only a few from another), the model can get a high accuracy score just by doing well at predicting the majority class (in the most extreme case the model could learn to only generate predictions for the majority class value). If our application is dependent on a particular class then it is important that we're able to judge the model's performance for that class (e.g. forest related applications require good performance of the tree class in land cover classifications).\n",
    "\n",
    "</details>\n",
    "\n",
    "### Error matrix\n",
    "\n",
    "A visual way of assessing a model's per-class performance is a confusion matrix. On a per-class level this lets us visualise true positives, true negatives, false positives and false negatives. The top-left to bottom-right diagonal of the confusion matrix captures the true positive predictions. Ideally, we want to see all of the model's fall on this diagonal and zero values in the off-diagonals. \n",
    "\n",
    "The documentation for `scikit-learn`'s confusion matrix tools is <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions\" target=\"_blank\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix\n",
    "\n",
    "# drop columns not needed for model development\n",
    "gdf_tmp = gdf_lc.drop(columns=[\"year\", \"geometry\"]).copy()\n",
    "X = gdf_tmp.drop(columns=[\"class\"])\n",
    "y = gdf_tmp.loc[:, \"class\"]\n",
    "\n",
    "# create train-test splits and standardise the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "lc_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = lc_scaler.transform(X_train)\n",
    "X_test_scaled = lc_scaler.transform(X_test)\n",
    "\n",
    "# train a classifier\n",
    "cls = MLPClassifier(hidden_layer_sizes=(10,), random_state=4, solver=\"sgd\", max_iter=500).fit(X_train_scaled, y_train)\n",
    "\n",
    "# generate test set predictions\n",
    "y_test_preds = cls.predict(X_test_scaled)\n",
    "\n",
    "# generate confusion matrix\n",
    "labels = [\"water\", \"mangrove\", \"bare earth\", \"built\", \"cropland\", \"grassland\", \"shrub\", \"tree\"]\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_preds, display_labels=labels, xticks_rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f5cde-395d-44c9-81a6-2ab59229decb",
   "metadata": {},
   "source": [
    "We can see that, in general, the predicted class labels in the test set match the true labels. There is some confusion between cropland, grassland, and shrubs and also between shrubs and trees. There is also some cropland confusion with bare earth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e1f08",
   "metadata": {},
   "source": [
    "### Per-class performance metrics\n",
    "\n",
    "We can also compute metrics that report on the model's performance for individual classes. \n",
    "\n",
    "The *precision metric* for a class is an indication of the model's ability to minimise false positives (i.e. how well the model does at avoiding assigning a class label to an example when it is not that class). In the remote sensing world, this metric is often called the *user's accuracy*. \n",
    "\n",
    "The *recall metric* for a class is an indication of the model's ability minimise false negatives (i.e. how well does the model do at assigning a class label to an example when it is that class). In the remote sensing world, this metric is often called *producer's accuracy*.  \n",
    "\n",
    "You can find a useful description of each of these metrics on <a href=\"https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall\" target=\"_blank\">Google's Machine Learning Course</a>.\n",
    "\n",
    "There is a helpful <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\" target=\"_blank\"><code>classification report</code></a> tool in `scikit-learn` that we can use to generate per-class performance metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf95c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ffc81-adeb-4246-aa56-3927f20b58e8",
   "metadata": {},
   "source": [
    "#### Activity!\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Look at the precision and recall scores in the classification report. Can you summarise how accurate the classifier is for individual classes?</summary>\n",
    "    \n",
    "The model has relatively high recall and precision scores for water, mangrove, built, cropland, and tree classes. For these classes, and this test set, the model is relatively robust to false positives and false negatives. Shrubs and bare earth have low recall scores, this indicates the model fails to classify examples in the test set as these classes when it should do. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4326b-4027-4dea-ba0f-079efa0672cc",
   "metadata": {},
   "source": [
    "#### Activity!\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Look up the F1-Score in the <code>scikit-learn</code> docs. Why is it a useful performance metric?</summary>\n",
    "    \n",
    "This ones on you. Go for it. \n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
