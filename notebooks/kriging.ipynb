{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd32800-552c-4f09-bce3-a674b943905f",
   "metadata": {},
   "source": [
    "# Regression kriging\n",
    "\n",
    "## Kriging\n",
    "\n",
    "Spatial interpolation is the process of estimating the target value at locations where we do not have observations using the values at nearby locations where there are observations. It aims to generate a complete surface of estimate target values from a sample of locations where the target variable is observed.\n",
    "\n",
    "One method to implement spatial interpolation is kriging, a geostatistical approach, which estimates the value at the unknown location as a weighted average of values at nearby locations. \n",
    "\n",
    "$$\n",
    "\\hat{z}(s_{0}) = \\sum_{i=1}^{n} \\gamma_{i}z(s_{i})\n",
    "$$\n",
    "\n",
    "where $\\hat{z}(s_{0})$ the estimated target value at the candidate location for prediction, $z(s_{i})$ are nearby locations where the target value is observed, and $\\gamma_{i}$ are weights. The weights are a function of the distance between observation $i$ and teh candidate location for prediction $0$ and the spatial autocorrelation structure of the data. Considering the spatial autocorrelation structure of the data makes kriging different from interpolation schemes that only consider distance weighted observations (e.g. inverse distance weighting). In kriging the spatial autocorrelation structure of the data is modelled using variograms; these will introduced and demonstrated in this notebook. \n",
    "\n",
    "## Predictive mapping and geospatial machine learning\n",
    "\n",
    "An alternative approach to generating a surface of estimated target values is to use a machine learning model that predicts the target value based on predictor variables with complete spatial coverage. A machine learning model is trained to learn relationships between input predictor variables and target values; a simple machine learning model is a regression model:\n",
    "\n",
    "$$\n",
    "\\hat{y} = b + w_{1}x_{1}\n",
    "$$\n",
    "\n",
    "Where the parameter values for $b$ and $w_{1}$ are learnt during training to minimise prediction error of the target value. This model can be applied everywhere there is coverage of the predictor variable $x_{1}$ to generate a predicted map of the target value. The advantage of predictive mapping using machine learning is a wide range of predictor variables that are correlated with the target value can be used in estimating the target value at unknown locations. Also, these predictor variables might include direct observations of conditions at the candidate location for prediction. This contrasts with kriging where conditions at the candidate site for prediction are inferred from a model of the spatial autocorrelation structure of the data and the distance to locations with observations of the target value.   \n",
    "\n",
    "## Regression kriging\n",
    "\n",
    "Regression kriging is a combination of predictive mapping using a machine learning model and interpolation via kriging. Regression kriging generates a predictive map via the following process: \n",
    "\n",
    "1. A machine learning model is developed using predictor variables with complete spatial coverage. \n",
    "2. The model is then used to generate predictions at locations where there are observations of the target value. \n",
    "3. The residuals are computed as the difference the predicted and observed target values. \n",
    "4. A complete spatial surface is predicted using the trained model and predictor variables. \n",
    "5. The residuals are interpolated using kriging.\n",
    "6. The predicted map and the residuals are added together to yield the final predicted surface. \n",
    "\n",
    "Regression kriging can be defined mathematically as:\n",
    "\n",
    "$$\n",
    "\\hat{z}(s_{0}) = b + w_{1}x_{0} + \\sum_{i=1}^{n} \\gamma_{i}z(s_{i})\n",
    "$$\n",
    "\n",
    "Where $x_{0}$ is the predictor variable at the candidate location for prediction $0$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f65242-117d-474b-b89b-570d1a6452e0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287b168-3ce1-4242-9fba-65b69d63ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "if \"data-geoml\" not in os.listdir(os.getcwd()):\n",
    "    subprocess.run('wget \"https://github.com/envt-5566/geo-ml/raw/main/data/data-geoml.zip\"', shell=True, capture_output=True, text=True)\n",
    "    subprocess.run('unzip \"data-geoml.zip\"', shell=True, capture_output=True, text=True)\n",
    "    if \"data-geoml.zip\" not in os.listdir(os.getcwd()):\n",
    "        print(\"Has a directory called data-geoml been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n",
    "    else:\n",
    "        print(\"Data download OK\")\n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b55de-229b-48e0-aa37-fa24b276281d",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19956ee6-ae15-4385-b4e6-8eb462723263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pykrige and gstools\n",
    "!pip install pykrige\n",
    "!pip install gstools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae17b2-aef5-4689-9715-c115a73624f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install xarray[complete]\n",
    "    !pip install rioxarray\n",
    "    !pip install mapclassify\n",
    "    !pip install contextily\n",
    "    !pip install pysal\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# geostatistics\n",
    "import gstools as gs\n",
    "from pykrige.rk import RegressionKriging\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import contextily as cx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# models\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ccc278-e9ee-41f1-a4e9-4d97533529af",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "We will be focusing on using regression kriging to generate a model that predicts plant species richness at a location using bioclimatic variables and elevation as predictors with complete spatial coverage (the machine learning model component of regression kriging) and observations of plant species richness at nearby locations (the spatial interpolation component of regression kriging). \n",
    "\n",
    "This is the dataset presented in <a href=\"https://arxiv.org/html/2404.06978v1\" target=\"_blank\">Meyer et al. (2024)</a> that includes points across South America representing vegetation surveys where species richness counts were recorded from the <a href=\"https://onlinelibrary.wiley.com/doi/10.1111/geb.13346\" target=\"_blank\">sPlotOpen database</a>. Species richness counts are the target values. Predictor variables included with this dataset are <a href=\"https://developers.google.com/earth-engine/datasets/catalog/WORLDCLIM_V1_BIO#bands\" target=\"_blank\">WorldClim bioclimatic variables</a> and elevation. The bioclimatic variables have the following definitions:\n",
    "\n",
    "* BIO1: Annual Mean Temperature\n",
    "* BIO2: Mean Diurnal Range (Mean of monthly (max temp - min temp))\n",
    "* BIO3: Isothermality (BIO2/BIO7) (×100)\n",
    "* BIO4: Temperature Seasonality (standard deviation ×100)\n",
    "* BIO5: Max Temperature of Warmest Month\n",
    "* BIO6: Min Temperature of Coldest Month\n",
    "* BIO7: Temperature Annual Range (BIO5-BIO6)\n",
    "* BIO8: Mean Temperature of Wettest Quarter\n",
    "* BIO9: Mean Temperature of Driest Quarter\n",
    "* BIO10: Mean Temperature of Warmest Quarter\n",
    "* BIO11: Mean Temperature of Coldest Quarter\n",
    "* BIO12: Annual Precipitation\n",
    "* BIO13: Precipitation of Wettest Month\n",
    "* BIO14: Precipitation of Driest Month\n",
    "* BIO15: Precipitation Seasonality (Coefficient of Variation)\n",
    "* BIO16: Precipitation of Wettest Quarter\n",
    "* BIO17: Precipitation of Driest Quarter\n",
    "* BIO18: Precipitation of Warmest Quarter\n",
    "* BIO19: Precipitation of Coldest Quarter\n",
    "\n",
    "Let's start by loading and mapping the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086052d-4019-46dc-bb7f-124e9fc8e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(os.path.join(DATA_PATH, \"plant_species_south_america.gpkg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b5f155-386d-4287-b69c-4e9d82339c6d",
   "metadata": {},
   "source": [
    "Let's explore the data on a web map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd60af-35fa-4775-a721-b1f5a77eb8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.explore(column=\"Species_richness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db586046-7a00-471e-a9b2-09b38b547f70",
   "metadata": {},
   "source": [
    "We can see the locations where we have observations of species richness are clustered within the extent of South America. The extent to which observations of species richness are spatially autocorrelated (i.e. locations close to each other have similar plant species richness values) will influence the effectiveness of kriging interpolation estimates of plant species richness at unknown locations.\n",
    "\n",
    "In kriging, semivariance is used to measure the spatial autocorrelation between observations and semivariograms are used to visualise the spatial autocorrelation structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc8430-724a-431f-8efd-c685f99b445d",
   "metadata": {},
   "source": [
    "## Variograms\n",
    "\n",
    "In kriging, the semivariance is used as a measure of spatial correlation.  The semivariance is defined as half the squared difference between the target value at two locations $h$ distance apart. \n",
    "\n",
    "$$\n",
    "\\gamma(h) = \\frac{1}{2}[z(x_{i}) - z(x_{j})]^2\n",
    "$$\n",
    "\n",
    "Where $\\gamma(h)$ is the semivariance and $z(x_{i})$ and $z(x_{j})$ are target values at locations $i$ and $j$ at $h$ distance apart. \n",
    "\n",
    "The empircal semivariogram plots the semivariance between all observations in a dataset against the distance between the observation locations. This visualises the spatial autocorrelation structure of the dataset; for example, spatially autocorrelated data examples that are close to each other in space will have small semivariance values as they would have similar values for the target variable. \n",
    "\n",
    "To aid interpretation of the semivariogram, often the data is binned by averaging the semivariance scores within certain distance ranges (bins). This can help make the spatial autocorrelation structure clearer amongst noise and scatter in the dataset.  \n",
    "\n",
    "We can use the <a href=\"https://geostat-framework.readthedocs.io/projects/gstools/en/stable/api/gstools.variogram.vario_estimate.html#gstools.variogram.vario_estimate\" target=\"_blank\">`vario_estimate()` function</a> from the <a href=\"https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/03_variogram/06_auto_bin_latlon.html\" target=\"_blank\">GSTools</a> package. \n",
    "\n",
    "### Data pre-processing\n",
    "\n",
    "In order to generate the variogram we need an array of the coordinates the locations of all the examples in our dataset (this lets us compute the distance between points) and a corresponding array of the target values. Here, we'll compute the semivariogram for the plant species richness observations across South America. \n",
    "\n",
    "The following code snippet demonstrates how to create an `x` and `y` column storing the longitude and latitude values for the points. We then subset out the `x` and `y` columns and the target variable `Species_richness` and store them as NumPy `ndarray`s. We also transpose the arrays so they are list-like (values aligned along the 0-axis). This is what the `T` operator does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09a1ef-5c7b-4e93-9529-c9cd650d3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.loc[:, \"x\"] = gdf.geometry.x\n",
    "gdf.loc[:, \"y\"] = gdf.geometry.y\n",
    "\n",
    "data = gdf.loc[:, \"Species_richness\"].to_numpy().T\n",
    "pos = gdf.loc[:, [\"x\", \"y\"]].to_numpy().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45019178-37f9-435a-8726-b2fa2b665446",
   "metadata": {},
   "source": [
    "### Variogram computation \n",
    "\n",
    "Next, we can pass `pos`, an array of coordinates, and `data`, an array of plant species richness values, into the <a href=\"https://geostat-framework.readthedocs.io/projects/gstools/en/stable/api/gstools.variogram.vario_estimate.html#gstools.variogram.vario_estimate\" target=\"_blank\">`vario_estimate()` function</a> to compute the empirical semivariogram. \n",
    "\n",
    "Note, we also set the `latlon` argument to `True` as the coordinates are geographic and we want our distance measurements between points to account for the curvature of the Earth. The `geo_scale` argument is also set to `gs.KM_SCALE` to get the variogram bins returned in km scaling and not radians (the units of latitude and longitude). This helps us interpret the semivariogram and the spatial autocorrelation structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9542cdd-c160-40de-904c-71a5c8400aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_v = gs.vario_estimate(pos, data, latlon=True, geo_scale=gs.KM_SCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b778a503-d0b3-4bd1-8c21-2199c6f1bb8e",
   "metadata": {},
   "source": [
    "The <a href=\"https://geostat-framework.readthedocs.io/projects/gstools/en/stable/api/gstools.variogram.vario_estimate.html#gstools.variogram.vario_estimate\" target=\"_blank\">`vario_estimate()` function</a> returns a tuple of two arrays. The first array stores the distance at the centre of bins and the second array stores the semivariance values. We can plot the empirical semivariogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67090815-7318-4f4e-9b4a-51a835716e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(5, 5)\n",
    "ax.scatter(emp_v[0], emp_v[1])\n",
    "ax.set_title(\"Empirical semivariogram for plant species richness\")\n",
    "ax.set_ylabel(\"semivariance\")\n",
    "ax.set_xlabel(\"distance (km)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45845d3e-7a4a-4544-8812-0fb4fb2e61ea",
   "metadata": {},
   "source": [
    "## Model variograms\n",
    "\n",
    "The model semivariogram is a mathematical model that best fits the computed semivariogram of the observations. A model is an estimate of the semivariance between two locations any $h$ distance apart, and, thus, it can be used to for guiding interpolation to generate a continuous surface. The empirical semivariogram only records the semivariance between locations where we have observations. However, to generate an interpolated surface we need an estimate of the semivariance at all potential distances between locations. In kriging, the semivariance as a measure of how similar a target value is for points $h$ distance apart is used to weight the contribution of observed values to predicting the value at an unknown location. Therefore, we fit a model to our empirical semivariogram that gives an estimate of the semivariance for all distances between points. \n",
    "\n",
    "The GSTools package has an <a href=\"https://geostat-framework.readthedocs.io/projects/gstools/en/stable/api/gstools.covmodel.Exponential.html#gstools.covmodel.Exponential\" target=\"_blank\"><code>Exponential()</code> function</a> that has a `fit()` method. We can pass the empirical semivariogram object into the fit method to estimate an exponential model of the semivariogram. Again, we need to account for the fact our data is in geographic coordinates. \n",
    "\n",
    "Model semivariograms are described by the nugget (the semivariance when the distance between points is 0), the sill (the semivariance at the distance between points when the semivariance stops increasing with distance), and the range (the distance at which the semivariance stops increasing or when the spatial corrrelation between points has diminished). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67bdbb3-dbce-4d63-81c4-dd1996ea5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = gs.Exponential(latlon=True, geo_scale=gs.KM_SCALE)\n",
    "exp.fit_variogram(*emp_v)\n",
    "ax = exp.plot(\"vario_yadrenko\", x_max=2 * np.max(emp_v[0]))\n",
    "ax.scatter(*emp_v)\n",
    "ax.legend()\n",
    "print(f\"Model variogram nugget: {exp.nugget}\")\n",
    "print(f\"Model variogram sill: {exp.sill}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77636cf3-8618-4e65-88c5-b6cfcf1212b8",
   "metadata": {},
   "source": [
    "## Regression kriging\n",
    "\n",
    "### Data pre-processing\n",
    "\n",
    "To train a regression kriging model we need three datasets: predictor variables, target values, and the coordinates where we have examples of predictor variables and observed target values. \n",
    "\n",
    "Our coordinate values will be in latitude and longitude units. A key part of kriging is measuring the distance between examples; the empirical semivariogram visualises how similar target values are for examples a given distance apart. As latitude and longitude values are geograhic coordinates (i.e. they are not projected coordinates onto a flat surface where we can measure distance in metres) we need to be careful to set up our regression kriging workflow so it is aware that coordinate values are geographic.\n",
    "\n",
    "The regression model component of regression kriging requires predictor variables and observations of the target value to train the model.\n",
    "\n",
    "The kriging component of a regression kriging model needs the locations (coordinates) of the training data and observations of the target value in order to model the spatial autocorrelation to generate kriging weights for spatial interpolation. \n",
    "\n",
    "We also need to split our data into train and test splits so we can evaluate our regression kriging model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cf2e7-7bc3-4503-8a66-79e200e9d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of coordinates\n",
    "# these are geographic coordinates (i.e. latitude and longitude)\n",
    "# we need to remember to set up our regression kriging workflow to\n",
    "# account for spherical coordinates\n",
    "pos = gdf.loc[:, [\"x\", \"y\"]].to_numpy()\n",
    "\n",
    "# drop columns not needed for model development\n",
    "gdf_tmp = gdf.drop(columns=[\"PlotObservationID\", \"GIVD_ID\", \"Country\", \"Biome\", \"geometry\", \"x\", \"y\"])\n",
    "X = gdf_tmp.drop(columns=[\"Species_richness\"]).to_numpy()\n",
    "y = gdf_tmp.loc[:, \"Species_richness\"].to_numpy()\n",
    "\n",
    "# set aside 30% of the data as a test split\n",
    "# set the random state for reproducibility\n",
    "X_train, X_test, pos_train, pos_test, y_train, y_test = train_test_split(X, pos, y, test_size=0.3, random_state=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c8b11e-9817-436a-be7b-cd42b9e65057",
   "metadata": {},
   "source": [
    "Next, we need to standardise our predictor variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41a71d-1840-4767-acc7-9f27cb37f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0988fa8-996b-4066-96f9-c0b4e96608ec",
   "metadata": {},
   "source": [
    "### Model development\n",
    "\n",
    "Regression kriging requires a model which is trained to learn relationships between predictor variables and target values. Let's setup a multi-layer perceptron model that we can train as part of the regression kriging workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada231d4-9987-49f8-939c-802f51bba211",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_rk = MLPRegressor(hidden_layer_sizes=(50,), random_state=4, solver=\"sgd\", max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f285a-ceda-44de-a3f3-a93bd8bf9b98",
   "metadata": {},
   "source": [
    "Next, we need to setup our regression kriging workflow. We create an instance of the `RegressionKriging` class from the `PyKrige` package. We pass in our model which will be trained to predict target values from input predictors and a series of parameters that define the kriging component. Here, we're using an exponential variogram model and we set the `coordinates_type` to `\"geographic\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32baffce-680d-4c3b-ae8a-64d62af10102",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rk = RegressionKriging(regression_model=regr_rk, n_closest_points=10, variogram_model=\"exponential\", coordinates_type=\"geographic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c573396-8504-4d3e-9862-bfe349bee34d",
   "metadata": {},
   "source": [
    "Similar to other `scikit-learn` models we have been working with, `RegressionKriging` objects have a `fit()` method. Calling `fit()` on `m_rk`, our `RegressionKriging` object, will do two things:\n",
    "\n",
    "* Train a model to predict target values from input predictor variables. This would be the same as in a normal machine learning workflow.\n",
    "* Fit a variogram model to the emprical semivariogram of the residuals of predictions obtained after training the model.\n",
    "* Krige the residuals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fce64-a554-48ef-a1e3-a0b198b24865",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rk.fit(X_train_scaled, pos_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe4c56-6c6b-470f-81d0-478e7f368840",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "We can evaluate our regression kriging model using the normal strategy of comparing model predictions of target values to observed values for a held out test set. We can use a trained `RegressionKriging` model object's `predict()` method to generate these predictions for the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe012a3-59f2-45b1-85fd-26ad85448df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rk_test_preds = m_rk.predict(X_test_scaled, pos_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd93815-2b63-4956-b8f0-b74eefc52aca",
   "metadata": {},
   "source": [
    "With predicted and observed target values for the test set, we can estimate the model's error using a performance metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac1d31-d798-4fd0-b064-f6bc3e7230d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = mean_squared_error(y_test, y_rk_test_preds)\n",
    "print(f\"The MSE on the test split is: {test_mse}\")\n",
    "print(f\"The RMSE on the test split is: {math.sqrt(test_mse)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a4c82-5abe-4e2e-87c4-7d8834f19116",
   "metadata": {},
   "source": [
    "Let's compare the regression kriging workflow to a normal machine learning workflow that does not consider the spatial relationship between examples in the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ed1eb-f108-4e0a-a84f-fd3af277d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(50,), random_state=4, solver=\"sgd\", max_iter=500)\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "y_test_preds = regr.predict(X_test_scaled)\n",
    "test_mse = mean_squared_error(y_test, y_test_preds)\n",
    "print(f\"The MSE on the test split is: {test_mse}\")\n",
    "print(f\"The RMSE on the test split is: {math.sqrt(test_mse)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2937b-54f2-46f6-8d6b-ed2b367deae0",
   "metadata": {},
   "source": [
    "#### Activity!\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Consider how we have evaluated our regression kriging model. Can you suggest a different strategy for evaluating the regression kriging model that accounts for the spatial structure of the training data?</summary>\n",
    "    \n",
    "The ideal strategy would be to generate a representative probability sample over the target area we wish to generate predicted map. However, it might be impossible to generate such a dataset. Alternatively, spatial k-fold cross-validation could be used to estimate model error when generating predictions far away from where the training data was collected. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49da21-4d08-4e63-b61a-0b0f6e40b7ed",
   "metadata": {},
   "source": [
    "#### Activity!\n",
    "\n",
    "Let's load another dataset consisting of points collected over the Marburg Forest in Germany and are from the paper by <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0304380019303230\" target=\"_blank\">Meyer et al. (2019)</a>. Each point has a lead area index (LAI) value and a series of predictor variables derived from a <a href=\"https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR_HARMONIZED\" target=\"_blank\">Sentinel-2 satellite image</a> and topographic variables. \n",
    "\n",
    "**Can you plot the empirical semivariogram for the LAI values?**\n",
    "\n",
    "**Can you fit a regression kriging model to predict LAI values based on the Sentinel-2 satellite observations, topographic variables, and kriging the residuals of a fitted machine learning model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1b416-903b-466c-b29e-9820ba33af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(os.path.join(DATA_PATH, \"lai_meyer_et_al_2019_marburg.geojson\"))\n",
    "gdf = gdf.drop(columns=[\"field_1\", \"ID\", \"x_utm_25832\", \"y_utm_25832\"])\n",
    "gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15b598-6e13-4e62-97bd-c5ff89c053c5",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Can you plot the empirical semivariogram for the LAI values?</summary>\n",
    "    \n",
    "```\n",
    "gdf.loc[:, \"x\"] = gdf.geometry.x\n",
    "gdf.loc[:, \"y\"] = gdf.geometry.y\n",
    "\n",
    "data = gdf.loc[:, \"LAI\"].to_numpy().T\n",
    "pos = gdf.loc[:, [\"x\", \"y\"]].to_numpy().T\n",
    "\n",
    "emp_v = gs.vario_estimate(pos, data, latlon=True, geo_scale=gs.KM_SCALE)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(5, 5)\n",
    "ax.scatter(emp_v[0], emp_v[1])\n",
    "ax.set_title(\"Empirical semivariogram for LAI\")\n",
    "ax.set_ylabel(\"semivariance\")\n",
    "ax.set_xlabel(\"distance (km)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddad2ad-4f34-4604-b0d9-9559a9e57a84",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Can you fit a regression kriging model to predict LAI values based on the Sentinel-2 satellite observations, topographic variables, and kriging the residuals of a fitted machine learning model?</summary>\n",
    "    \n",
    "```\n",
    "# create an array of coordinates\n",
    "# these are geographic coordinates (i.e. latitude and longitude)\n",
    "# we need to remember to set up our regression kriging workflow to\n",
    "# account for spherical coordinates\n",
    "pos = gdf.loc[:, [\"x\", \"y\"]].to_numpy()\n",
    "\n",
    "# drop columns not needed for model development\n",
    "gdf_tmp = gdf.drop(columns=[\"geometry\", \"x\", \"y\"])\n",
    "X = gdf_tmp.drop(columns=[\"LAI\"]).to_numpy()\n",
    "y = gdf_tmp.loc[:, \"LAI\"].to_numpy()\n",
    "\n",
    "# set aside 30% of the data as a test split\n",
    "# set the random state for reproducibility\n",
    "X_train, X_test, pos_train, pos_test, y_train, y_test = train_test_split(X, pos, y, test_size=0.3, random_state=4) \n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# setup and fit the regression kriging model\n",
    "regr_rk = MLPRegressor(hidden_layer_sizes=(50,), random_state=4, solver=\"sgd\", max_iter=500)\n",
    "m_rk = RegressionKriging(regression_model=regr_rk, n_closest_points=10, variogram_model=\"exponential\", coordinates_type=\"geographic\")\n",
    "m_rk.fit(X_train_scaled, pos_train, y_train)\n",
    "\n",
    "# predict LAI at test location\n",
    "y_rk_test_preds = m_rk.predict(X_test_scaled, pos_test)\n",
    "\n",
    "# model evaluation\n",
    "test_mse = mean_squared_error(y_test, y_rk_test_preds)\n",
    "print(f\"The MSE on the test split is: {test_mse}\")\n",
    "print(f\"The RMSE on the test split is: {math.sqrt(test_mse)}\")\n",
    "```\n",
    " \n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
